{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, sys, signal,rospy, argparse, csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.distance import cdist\n",
    "import copy, pickle\n",
    "import open3d as o3d\n",
    "import open3d.visualization as vis\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from open3d_ros_helper import open3d_ros_helper as orh\n",
    "from geometry_msgs.msg import Pose, PoseArray, Point # PoseArray, Pose\n",
    "\n",
    "from models.mdgat import MDGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='Point cloud matching and pose evaluation',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    slam_dir = '/media/vision/Seagate/DataSets/KRGM/kitti/',\n",
    "    data_folder = 'harris_3D',\n",
    "    local_global = False,\n",
    "    seq_num = '00',\n",
    "    visualize = False,\n",
    "    vis_line_width = 0.2,\n",
    "    calculate_pose = True,\n",
    "    learning_rate = 0.0001,\n",
    "    batch_size = 1,\n",
    "    train_path = './KITTI/',\n",
    "    model_out_path = './models/checkpoint',\n",
    "    memory_is_enough = True,\n",
    "    local_rank = 0,\n",
    "    txt_path = './KITTI/preprocess-random-full',\n",
    "    keypoints_path = './KITTI/keypoints/tsf_256_FPFH_16384-512-k1k16-2d-nonoise',\n",
    "    resume_model = './checkpoint/kitti/mdgat-l9-gap_loss-pointnetmsg-04_01_19_32/train_step3/nomutualcheck-mdgat-batch16-gap_loss-pointnetmsg-USIP-04_01_19_32/best_model_epoch_221(val_loss0.31414552026539594).pth',\n",
    "    loss_method = 'triplet_loss',\n",
    "    net = 'mdgat',\n",
    "    mutual_check = False,\n",
    "    k = [128, None, 128, None, 64, None, 64, None],\n",
    "    l = 9,\n",
    "    descriptor = 'pointnetmsg',\n",
    "    keypoints = 'USIP',\n",
    "    ensure_kpts_num = False,\n",
    "    max_keypoints = -1,\n",
    "    match_threshold = 0.2,\n",
    "    threshold = 0.5,\n",
    "    triplet_loss_gamma = 0.5,\n",
    "    sinkhorn_iterations = 20,\n",
    "    train_step = 3,\n",
    "\n",
    "    fpfh_normal_radiuse = 0.3,\n",
    "    fpfh_descriptors_radiuse = 1.0,\n",
    "    seq_list = [0],\n",
    "    mdgat_path = './KITTI',\n",
    "    kitti_path = '/media/vision/Seagate/DataSets/kitti/dataset/sequences',\n",
    "    transform_opt = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    def __init__(self, args) -> None:\n",
    "        self.gt_seq = args.seq_num\n",
    "        self.data_folder = args.data_folder\n",
    "\n",
    "        self.dir_SLAM_path = args.slam_dir + self.data_folder +\"/\"\n",
    "        self.descriptor_type = args.descriptor\n",
    "\n",
    "        self.poses = []\n",
    "        self.dense_scans = []\n",
    "        self.keypoints = []\n",
    "        self.descriptors = []\n",
    "        self.local_graph_range = [0, 0]\n",
    "        self.divided_keypoints = np.array([])\n",
    "        self.divided_dense_scans = []\n",
    "        self.divided_seq_num = 0\n",
    "\n",
    "        self._get_SLAM_poses()\n",
    "        self._get_dense_frames()\n",
    "        self._get_keypoints()\n",
    "        self._get_descriptors()\n",
    "        print(\"[Load] %d's poses SLAM data loaded\" % len(self.poses))\n",
    "\n",
    "    def _get_SLAM_poses(self):\n",
    "        with open(file=os.path.join(self.dir_SLAM_path, \"Poses_kitti_\" + self.gt_seq + \".pickle\"), mode='rb') as f:\n",
    "            self.poses = pickle.load(f)\n",
    "\n",
    "    def _get_dense_frames(self):\n",
    "        with open(file=os.path.join(self.dir_SLAM_path, \"DenseFrames_kitti_\" + self.gt_seq + \".pickle\"), mode='rb') as f:\n",
    "            self.dense_scans = pickle.load(f)\n",
    "\n",
    "    def _get_keypoints(self):\n",
    "        with open(file=os.path.join(self.dir_SLAM_path, \"keyPoints_kitti_\" + self.gt_seq + \".pickle\"), mode='rb') as f:\n",
    "            self.keypoints = pickle.load(f) # (pose_num, keypoint_num, 3), (keypoint_num, 3)는 np.array\n",
    "\n",
    "    def _get_descriptors(self):\n",
    "        if self.descriptor_type == \"FPFH\":\n",
    "            with open(file=os.path.join(self.dir_SLAM_path, \"Descriptors_FPFH_kitti_\" + self.gt_seq + \".pickle\"), mode='rb') as f:\n",
    "                self.descriptors = pickle.load(f)\n",
    "        elif self.descriptor_type == \"pointnet\" or \"pointnetmsg\":\n",
    "            for kps in self.keypoints:\n",
    "                self.descriptors.append(torch.zeros((kps.shape[0], 128)))\n",
    "\n",
    "    def divide_data(self, divide_num = 256):\n",
    "        # divide keypoints\n",
    "        flatten_keypoints = []\n",
    "        for kps in self.keypoints:\n",
    "            for kp in kps:\n",
    "                flatten_keypoints.append(np.array(kp))\n",
    "        flatten_keypoints = np.array(flatten_keypoints) # (n, 3)\n",
    "        sample_num = flatten_keypoints.shape[0] // (divide_num)\n",
    "        flatten_keypoints = flatten_keypoints[:(divide_num) * sample_num]\n",
    "        self.divided_keypoints = flatten_keypoints.reshape((sample_num, divide_num, 3))\n",
    "\n",
    "        # devide dense_scans\n",
    "        n = 0\n",
    "        divided_pcd = o3d.geometry.PointCloud()\n",
    "        for idx, kps in enumerate(self.keypoints):\n",
    "            divided_pcd.points.extend(self.dense_scans[idx])\n",
    "            for _ in kps:\n",
    "                n += 1\n",
    "                if n % divide_num == 0:\n",
    "                    divided_pcd = divided_pcd.voxel_down_sample(voxel_size=0.2)\n",
    "                    self.divided_dense_scans.append(np.array(divided_pcd.points))\n",
    "                    divided_pcd.clear()\n",
    "                    divided_pcd.points.extend(self.dense_scans[idx])\n",
    "                    n = 0\n",
    "\n",
    "        self.divided_seq_num = sample_num\n",
    "        self.descriptors = [None] * sample_num\n",
    "        print(\"[Divide] Keypoints and dense scan divided into %d-num seq\" % (sample_num))\n",
    "\n",
    "    def get_divided_data(self, idx):\n",
    "        if idx >= len(self.divided_keypoints) - 1:\n",
    "            idx = len(self.divided_keypoints) - 1\n",
    "            print(\"[Warning] idx is over the divided keypoints, so return the last one\")\n",
    "        infer_kp = torch.tensor(self.divided_keypoints[idx], dtype=torch.double).unsqueeze(0)\n",
    "        infer_pc = torch.tensor(self.divided_dense_scans[idx], dtype=torch.double)\n",
    "        infer_pc = torch.cat((infer_pc, torch.ones(infer_pc.shape[0], 1)), dim=1)\n",
    "        \n",
    "        n = infer_pc.shape[0] // 2\n",
    "        infer_pc = infer_pc[:n*2,:]\n",
    "        infer_pc = infer_pc.reshape((-1, 8)).unsqueeze(0)\n",
    "        \n",
    "        infer_scores = torch.ones((infer_kp.shape[1])).unsqueeze(0)\n",
    "        \n",
    "        return{\n",
    "            'keypoints0': infer_kp,\n",
    "            'cloud0': infer_pc,\n",
    "            'scores0': infer_scores\n",
    "        } \n",
    "    \n",
    "    def push_descriptor(self, idx, descriptors):\n",
    "        if idx >= len(self.descriptors):\n",
    "            idx = len(self.descriptors) - 1\n",
    "            print(\"[Warning] idx is over the divided keypoints, so return the last one\")\n",
    "        \n",
    "        self.descriptors[idx] = descriptors.cpu()\n",
    "\n",
    "    def get_matching_data(self, idx):\n",
    "        if idx >= len(self.divided_keypoints) - 1:\n",
    "            idx = len(self.divided_keypoints) - 1\n",
    "            print(\"[Warning] idx is over the divided keypoints, so return the last one\")\n",
    "        if idx < 2:\n",
    "            idx = 2\n",
    "            print(\"[Warning] idx's minimum value is 2, so return same data with input idx 2\")\n",
    "\n",
    "        local_keypoints = self.divided_keypoints[idx]\n",
    "        local_descriptors = self.descriptors[idx]\n",
    "        local_pc = self.divided_dense_scans[idx]\n",
    "\n",
    "        for i in [idx + 1]:\n",
    "            local_keypoints = np.vstack((local_keypoints, self.divided_keypoints[i]))\n",
    "            local_descriptors = torch.cat((local_descriptors, self.descriptors[i]), dim=2)\n",
    "            local_pc = np.vstack((local_pc, self.divided_dense_scans[i]))\n",
    "\n",
    "        global_keypoints = self.divided_keypoints[0]\n",
    "        global_descriptors = self.descriptors[0]\n",
    "        global_pc = self.divided_dense_scans[0]\n",
    "\n",
    "        # for i in range(1, 2):\n",
    "        for i in [1]:\n",
    "            global_keypoints = np.vstack((global_keypoints, self.divided_keypoints[i]))\n",
    "            global_descriptors = torch.cat((global_descriptors, self.descriptors[i]), dim=2)\n",
    "            global_pc = np.vstack((global_pc, self.divided_dense_scans[i]))\n",
    "\n",
    "        local_pc = torch.tensor(local_pc, dtype=torch.double)\n",
    "        local_pc = torch.cat((local_pc, torch.ones(local_pc.shape[0], 1)), dim=1)\n",
    "        global_pc = torch.tensor(global_pc, dtype=torch.double)\n",
    "        global_pc = torch.cat((global_pc, torch.ones(global_pc.shape[0], 1)), dim=1)\n",
    "\n",
    "        dists = cdist(global_keypoints, local_keypoints)\n",
    "        min1 = np.argmin(dists, axis=0)\n",
    "        min2 = np.argmin(dists, axis=1)\n",
    "        min1v = np.min(dists, axis=1)\n",
    "        min1f = min2[min1v < 0.5]\n",
    "        rep = len(min1f)\n",
    "        match1, match2 = -1 * np.ones((len(global_keypoints)), dtype=np.int16), -1 * np.ones((len(local_keypoints)), dtype=np.int16)\n",
    "        match1[min1v < 0.5] = min1f\n",
    "        min2v = np.min(dists, axis=0)\n",
    "        min2f = min1[min2v < 0.5]\n",
    "        match2[min2v < 0.5] = min2f\n",
    "\n",
    "        global_keypoints = torch.tensor(global_keypoints, dtype=torch.double).unsqueeze(0)\n",
    "        local_keypoints = torch.tensor(local_keypoints, dtype=torch.double).unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            'keypoints0': global_keypoints,\n",
    "            'keypoints1': local_keypoints,\n",
    "            'descriptors0': global_descriptors,\n",
    "            'descriptors1': local_descriptors,\n",
    "            'cloud0': global_pc,\n",
    "            'cloud1': local_pc,\n",
    "            'gt_matches0': match1,\n",
    "            'gt_matches1': match2,\n",
    "            'rep': rep\n",
    "        }\n",
    "                                     \n",
    "    def __len__(self):\n",
    "        return len(self.poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emplimentation 테스트 시나리오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] 929's poses SLAM data loaded\n",
      "[Divide] Keypoints and dense scan divided into 42-num seq\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 및 분할\n",
    "dataset = dataset(opt)\n",
    "dataset.divide_data(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 3]) torch.Size([1, 54621, 8]) torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "pred = dataset.get_divided_data(2)\n",
    "print(pred['keypoints0'].shape, pred['cloud0'].shape, pred['scores0'].shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MDGAT(\n",
       "    (penc): PointnetEncoderMsg(\n",
       "      (sa1): PointNetSetKptsMsg(\n",
       "        (conv_blocks): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (bn_blocks): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (sa2): PointNetSetAbstraction(\n",
       "        (mlp_convs): ModuleList(\n",
       "          (0): Conv2d(323, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (mlp_bns): ModuleList(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (kenc): KeypointEncoder(\n",
       "        (encoder): Sequential(\n",
       "          (0): Conv1d(4, 32, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "          (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): ReLU()\n",
       "          (9): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (gnn): AttentionalGNN(\n",
       "      (layers): ModuleList(\n",
       "        (0): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (3): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (4): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (5): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (6): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (7): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (8): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (9): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (10): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (11): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (12): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (13): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (14): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (15): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (16): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (17): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_proj): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 모델 로드\n",
    "path_checkpoint = opt.resume_model  \n",
    "checkpoint = torch.load(path_checkpoint, map_location={'cuda:2':'cuda:0'})  \n",
    "lr = checkpoint['lr_schedule']\n",
    "config = {\n",
    "    'net': {\n",
    "        'sinkhorn_iterations': opt.sinkhorn_iterations,\n",
    "        'match_threshold': opt.match_threshold,\n",
    "        'lr': opt.learning_rate,\n",
    "        'loss_method': opt.loss_method,\n",
    "        'k': opt.k,\n",
    "        'descriptor': opt.descriptor,\n",
    "        'mutual_check': opt.mutual_check,\n",
    "        'triplet_loss_gamma': opt.triplet_loss_gamma,\n",
    "        'train_step':opt.train_step,\n",
    "        'L':opt.l\n",
    "    }\n",
    "}\n",
    "net = MDGAT(config.get('net', {}))\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=config.get('net', {}).get('lr'))\n",
    "net = torch.nn.DataParallel(net)\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # torch.cuda.set_device(opt.local_rank)\n",
    "    device=torch.device('cuda:{}'.format(opt.local_rank))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"### CUDA not available ###\")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dataset.get_matching_data(13)\n",
    "print(pred.keys())\n",
    "print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape, pred['scores0'].shape, pred['scores1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 디스크립터 추출\n",
    "net.double().eval()\n",
    "\n",
    "for i in range(dataset.divided_seq_num - 1):\n",
    "    pred = dataset.get_divided_data(i)\n",
    "    for p in pred:\n",
    "        pred[p] = pred[p].to(device)\n",
    "    data = net.module.infer_desc(pred)\n",
    "    for d in data:\n",
    "        data[d] = data[d].detach().cpu()\n",
    "    dataset.push_descriptor(i, data['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 3]) torch.Size([1, 512, 3]) torch.Size([1, 128, 512]) torch.Size([1, 128, 512])\n",
      "pred's keys:  dict_keys(['keypoints0', 'keypoints1', 'descriptors0', 'descriptors1', 'cloud0', 'cloud1', 'gt_matches0', 'gt_matches1', 'rep', 'matches0', 'matches1', 'matching_scores0', 'matching_scores1'])\n"
     ]
    }
   ],
   "source": [
    "# 4. 매칭\n",
    "# pred = dataset.get_matching_data(12)\n",
    "pred = dataset.get_matching_data(13)\n",
    "# pred = dataset.get_matching_data(12)\n",
    "for p in pred:\n",
    "    if type(pred[p]) == torch.Tensor:\n",
    "        pred[p] = pred[p].to(device)\n",
    "print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape)\n",
    "\n",
    "data = net.module.infer_mdgat(pred, [pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//4, None, pred['keypoints0'].shape[1]//4, None], [pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//4, None, pred['keypoints1'].shape[1]//4, None])\n",
    "pred = {**pred, **data}\n",
    "print(\"pred's keys: \", pred.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "valid = matches0 > -1\n",
    "mkpts0 = kpts0[valid]\n",
    "mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "mutual1 = matches0[mutual0]\n",
    "x = np.ones(len(matches1)) == 1\n",
    "x[mutual1] = False\n",
    "valid1 = matches1 > -1\n",
    "\n",
    "mconf = conf[valid]\n",
    "\n",
    "matches_gt, matches_gt1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "matches_gt[matches_gt == len(matches_gt1)] = -1\n",
    "matches_gt1[matches_gt1 == len(matches_gt)] = -1\n",
    "valid_gt = matches_gt > -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(matches0.shape, matches1.shape, conf.shape) # >> (3328,) (256,) (3328,)\n",
    "\n",
    "print(matches0[669]) # matches0의 669번째 인덱스와의 매칭 결과 >> 1\n",
    "\n",
    "print(matches1[:]) # matches1의 전체 결과 >> [  -1  669   -1  677 ...], 즉 matches1의 1번째 포인트가 matches0의 669번째 포인트와 매칭됨\n",
    "\n",
    "print(mutual0) # >> [   9   10   13   15 ...]\n",
    "\n",
    "print(mutual1) # >> [248 245  20 255 106 ...]\n",
    "\n",
    "print(matches0[9], matches1[248]) # >> 248, 9, 즉 mutual0[i]와 mutual1[i]의 idx끼리 매칭 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "lines = []\n",
    "for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "    # lines.append([match, match + 1])\n",
    "    # lines.append([mutual1[idx] + len(kpts0), mutual1[idx] + len(kpts0)])\n",
    "    lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "colors = [[0, 1, 0] for _ in range(len(lines))] # lines are shown in green\n",
    "# print(points[lines[0][0]], points[lines[0][1]])\n",
    "# print(pcd_kp0.points[lines[0][0]], pcd_kp1.points[lines[0][1] - len(pcd_kp0.points)])\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector((points)),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = 2.0\n",
    "\n",
    "# 시각화\n",
    "kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "desc0 = pred['descriptors0'][0].cpu().detach().numpy().T\n",
    "desc1 = pred['descriptors1'][0].cpu().detach().numpy().T\n",
    "dists = cdist(desc0, desc1)\n",
    "min0 = np.argmin(dists, axis=0)\n",
    "min1 = np.argmin(dists, axis=1)\n",
    "min0v = np.min(dists, axis=1)\n",
    "min0f = min1[min0v < ttt]\n",
    "match0, match1 = -1 * np.ones((len(desc0)), dtype=np.int16), -1 * np.ones((len(desc1)), dtype=np.int16)\n",
    "match0[min0v < ttt] = min0f\n",
    "min1v = np.min(dists, axis=0)\n",
    "min1f = min0[min1v < ttt]\n",
    "match1[min1v < ttt] = min1f\n",
    "mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "mutual1 = matches0[mutual0]\n",
    "\n",
    "# 시각화\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "lines = []\n",
    "for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "    lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "colors = [[0, 1, 0] for _ in range(len(lines))] # lines are shown in green\n",
    "# print(points[lines[0][0]], points[lines[0][1]])\n",
    "# print(pcd_kp0.points[lines[0][0]], pcd_kp1.points[lines[0][1] - len(pcd_kp0.points)])\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector((points)),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mutual0)\n",
    "print(mutual1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
