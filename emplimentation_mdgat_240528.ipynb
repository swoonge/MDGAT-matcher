{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, sys, signal,rospy, argparse, csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.distance import cdist\n",
    "import copy, pickle\n",
    "import open3d as o3d\n",
    "import open3d.visualization as vis\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from open3d_ros_helper import open3d_ros_helper as orh\n",
    "from geometry_msgs.msg import Pose, PoseArray, Point # PoseArray, Pose\n",
    "\n",
    "from models.mdgat_Rops2 import MDGAT\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='Point cloud matching and pose evaluation',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    dataset_dir = '/media/vision/Seagate/DataSets/denseKITTI',\n",
    "    # slam_dir = '',\n",
    "    data_folder = 'harris_3D',\n",
    "    local_global = False,\n",
    "    seq_num = 0,\n",
    "    visualize = False,\n",
    "    vis_line_width = 0.2,\n",
    "    calculate_pose = True,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 1,\n",
    "    train_path = './denseKITTI/',\n",
    "    model_out_path = './models/checkpoint',\n",
    "    memory_is_enough = True,\n",
    "    local_rank = 0,\n",
    "    txt_path = './KITTI/preprocess-random-full',\n",
    "    keypoints_path = './denseKITTI/keypoints',\n",
    "    resume_model = './checkpoint/denseKITTI/mdgat-l9-gap_loss-Rops-05_27_17_55/nomutualcheck-mdgat-batch128-lr0.001-gap_loss-Rops-Hariss3D-05_27_17_55/best_model_epoch_233(val_loss0.07754801417465815).pth',\n",
    "    loss_method = 'triplet_loss',\n",
    "    net = 'mdgat',\n",
    "    mutual_check = False,\n",
    "    k = [128, None, 128, None, 64, None, 64, None],\n",
    "    l = 9,\n",
    "    descriptor = 'Rops',\n",
    "    keypoints = 'harris_3D',\n",
    "    ensure_kpts_num = False,\n",
    "    max_keypoints = -1,\n",
    "    match_threshold = 0.2,\n",
    "    threshold = 0.5,\n",
    "    triplet_loss_gamma = 0.5,\n",
    "    sinkhorn_iterations = 100,\n",
    "    train_step = 3,\n",
    "\n",
    "    fpfh_normal_radiuse = 0.3,\n",
    "    fpfh_descriptors_radiuse = 1.0,\n",
    "    seq_list = [0],\n",
    "    mdgat_path = './KITTI',\n",
    "    kitti_path = '/media/vision/Seagate/DataSets/kitti/dataset/sequences',\n",
    "    transform_opt = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kitti_Rops_dataset_loader():\n",
    "    def __init__(self, args) -> None:\n",
    "        self.dataset_dir = args.dataset_dir\n",
    "        self.seq = args.seq_num\n",
    "        self.descriptor_type = args.descriptor\n",
    "\n",
    "        self.gt_pairs = []\n",
    "        self.poses = []\n",
    "        self.keypoints = []\n",
    "        self.scores = []\n",
    "        self.descriptors = []\n",
    "        self.dense_scans = []\n",
    "\n",
    "        self.local_graph_range = [0, 0]\n",
    "        self.divided_keypoints = np.array([])\n",
    "        self.divided_dense_scans = []\n",
    "        self.divided_seq_num = 0\n",
    "\n",
    "        self._load_gt_pairs()\n",
    "        self._load_datas()\n",
    "        print(\"[Load] %d's poses SLAM data loaded\" % len(self.poses))\n",
    "    \n",
    "    def _load_gt_pairs(self):\n",
    "        file_path = os.path.join(self.dataset_dir, 'groundtruths128', '%02d'%self.seq, 'groundtruths.txt')\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines_list = f.readlines()\n",
    "            for i, line_str in enumerate(lines_list):\n",
    "                if i == 0:\n",
    "                    continue # skip the header line\n",
    "                line_splitted = line_str.split()\n",
    "                anc_idx = int(float(line_splitted[0]))\n",
    "                pos_idx = int(float(line_splitted[1]))\n",
    "\n",
    "                data = {'seq': self.seq, 'anc_idx': anc_idx, 'pos_idx': pos_idx}\n",
    "                self.gt_pairs.append(data)\n",
    "\n",
    "    def _load_datas(self):\n",
    "        # poses\n",
    "        pose_path = os.path.join(self.dataset_dir, 'poses', '%02d.txt'%self.seq)\n",
    "        with open(pose_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                T_w_cam0 = np.fromstring(line, dtype=float, sep=' ')\n",
    "                T_w_cam0 = T_w_cam0.reshape(3, 4)\n",
    "                T_w_cam0 = np.vstack((T_w_cam0, [0, 0, 0, 1]))\n",
    "                self.poses.append(T_w_cam0)\n",
    "\n",
    "        # keypoints, scores and descriptors\n",
    "        keypoint_folder = os.path.join(self.dataset_dir, 'keypoints', '%02d'%self.seq)\n",
    "        keypoint_folder = os.listdir(keypoint_folder)   \n",
    "        keypoint_folder.sort(key=lambda x:int(x[:-4]))\n",
    "        for idx in range(len(keypoint_folder)):\n",
    "            file = os.path.join(self.dataset_dir, 'keypoints', '%02d'%self.seq, keypoint_folder[idx])\n",
    "            if os.path.isfile(file):\n",
    "                pc = np.reshape(np.fromfile(file, dtype=np.float64), (-1, 139))\n",
    "                self.keypoints.append(pc[:, :3])\n",
    "                self.scores.append(pc[:, 3])\n",
    "                self.descriptors.append(pc[:, 4:])\n",
    "            else:\n",
    "                self.keypoints.append([0.0, 0.0, 0.0])\n",
    "                self.scores.append([0])\n",
    "                self.descriptors.append([0.0]*135)\n",
    "\n",
    "        # dense scans\n",
    "        dense_folder = os.path.join(self.dataset_dir, 'dense_scan', '%02d'%self.seq)\n",
    "        dense_folder = os.listdir(dense_folder)\n",
    "        dense_folder.sort(key=lambda x:int(x[:-4]))\n",
    "        for idx in range(len(dense_folder)):\n",
    "            file = os.path.join(self.dataset_dir, 'dense_scan', '%02d'%self.seq, dense_folder[idx])\n",
    "            if os.path.isfile(file):\n",
    "                self.dense_scans.append(np.reshape(np.fromfile(file, dtype=np.float64), (-1, 3)))\n",
    "            else:\n",
    "                self.dense_scans.append(np.array([0, 0, 0]))\n",
    "\n",
    "    def get_gt_pairs(self, idx):\n",
    "        index_in_seq0 = self.gt_pairs[idx]['anc_idx']\n",
    "        index_in_seq1 = self.gt_pairs[idx]['pos_idx']\n",
    "\n",
    "        pose0 = torch.tensor(self.poses[index_in_seq0], dtype=torch.double)\n",
    "        pose1 = torch.tensor(self.poses[index_in_seq1], dtype=torch.double)\n",
    "        T_gt = torch.einsum('ab,de->ae', torch.inverse(pose0), pose1)\n",
    "\n",
    "        pc0 = self.dense_scans[index_in_seq0]\n",
    "        pc1 = self.dense_scans[index_in_seq1]\n",
    "\n",
    "        kp0_np = np.array([(kp[0], kp[1], kp[2], 1) for kp in self.keypoints[index_in_seq0]]) \n",
    "        kp1_np = np.array([(kp[0], kp[1], kp[2], 1) for kp in self.keypoints[index_in_seq1]])\n",
    "        kp0_tensor = torch.tensor(kp0_np, dtype=torch.double)\n",
    "        kp1_tensor = torch.tensor(kp1_np, dtype=torch.double)\n",
    "\n",
    "        kp0_local_tensor = torch.einsum('ij,nj->ni', torch.inverse(pose0), kp0_tensor).double()\n",
    "        kp1_local_tensor = torch.einsum('ij,nj->ni', torch.inverse(pose1), kp1_tensor).double()\n",
    "\n",
    "        desc0 = self.descriptors[index_in_seq0]\n",
    "        desc1 = self.descriptors[index_in_seq1]\n",
    "\n",
    "        kp0_num = len(kp0_tensor)\n",
    "        kp1_num = len(kp1_tensor)\n",
    "\n",
    "        norm0, norm1 = np.linalg.norm(desc0, axis=1), np.linalg.norm(desc1, axis=1)\n",
    "        norm0, norm1 = norm0.reshape(kp0_num, 1), norm1.reshape(kp1_num, 1)\n",
    "        epsilon = 1e-8  # small constant to prevent division by zero\n",
    "        norm0, norm1 = norm0 + epsilon, norm1 + epsilon\n",
    "        desc0, desc1 = np.where(norm0 != 0, np.multiply(desc0, 1/norm0), 0), np.where(norm1 != 0, np.multiply(desc1, 1/norm1), 0)\n",
    "\n",
    "        desc0_tensor, desc1_tensor = torch.tensor(desc0, dtype=torch.double), torch.tensor(desc1, dtype=torch.double)\n",
    "        scores0_tensor, scores1_tensor = torch.tensor(self.scores[index_in_seq0], dtype=torch.double), torch.tensor(self.scores[index_in_seq1], dtype=torch.double)\n",
    "\n",
    "        dists = cdist(kp0_tensor, kp1_tensor)\n",
    "        '''Find ground true keypoint matching'''\n",
    "        min1 = np.argmin(dists, axis=0)\n",
    "        min2 = np.argmin(dists, axis=1)\n",
    "        min1v = np.min(dists, axis=1)\n",
    "        min1f = min2[min1v < 0.5]\n",
    "\n",
    "        '''For calculating repeatibility'''\n",
    "        rep = len(min1f)\n",
    "\n",
    "        match1, match2 = -1 * np.ones((len(kp0_tensor)), dtype=np.int16), -1 * np.ones((len(kp1_tensor)), dtype=np.int16)\n",
    "        match1[min1v < 0.5] = min1f\n",
    "        min2v = np.min(dists, axis=0)\n",
    "        min2f = min1[min2v < 0.5]\n",
    "        match2[min2v < 0.5] = min2f\n",
    "            \n",
    "        # print(kp0_tensor.shape)\n",
    "        # print(kp0_tensor[:,:3].shape)\n",
    "\n",
    "        return{\n",
    "            # 'skip': False,\n",
    "            'keypoints0': kp0_local_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints1': kp1_local_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints_global_0': kp0_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints_global_1': kp1_tensor[:,:3].unsqueeze(0),\n",
    "            'descriptors0': desc0_tensor.unsqueeze(0),\n",
    "            'descriptors1': desc1_tensor.unsqueeze(0),\n",
    "            'scores0': scores0_tensor.unsqueeze(0),\n",
    "            'scores1': scores1_tensor.unsqueeze(0),\n",
    "            'gt_matches0': match1,\n",
    "            'gt_matches1': match2,\n",
    "            'sequence': self.seq,\n",
    "            'idx0': index_in_seq0,\n",
    "            'idx1': index_in_seq1,\n",
    "            'pose1': pose0,\n",
    "            'pose2': pose1,\n",
    "            # 'T_cam0_velo': T_cam0_velo,\n",
    "            'T_gt': T_gt,\n",
    "            'cloud0': pc0,\n",
    "            'cloud1': pc1,\n",
    "            # 'all_matches': list(all_matches),\n",
    "            # 'file_name': file_name\n",
    "            'rep': rep\n",
    "        }\n",
    "    \n",
    "    def process_global_keypoints(self):\n",
    "        pass\n",
    "    \n",
    "    def matching_test_1to2(self, idx, range_of_global_graph = 50, range_of_local_graph = 10):\n",
    "        index_in_seq0 = self.gt_pairs[idx]['anc_idx']\n",
    "        index_in_seq1 = self.gt_pairs[idx]['pos_idx']\n",
    "        while index_in_seq0 <= range_of_global_graph:\n",
    "            idx+=1\n",
    "            index_in_seq0 = self.gt_pairs[idx]['anc_idx']\n",
    "            index_in_seq1 = self.gt_pairs[idx]['pos_idx']\n",
    "            \n",
    "        pose0 = torch.tensor(self.poses[index_in_seq0], dtype=torch.double)\n",
    "        pose1 = torch.tensor(self.poses[index_in_seq1], dtype=torch.double)\n",
    "        T_gt = torch.einsum('ab,de->ae', torch.inverse(pose0), pose1)\n",
    "\n",
    "        pc0_o3d = o3d.geometry.PointCloud()\n",
    "        for i in range(index_in_seq0-range_of_global_graph, index_in_seq0):\n",
    "            pc0_o3d.points.extend(self.dense_scans[i])\n",
    "        pc0_o3d.voxel_down_sample(voxel_size=0.2)\n",
    "        pc0 = np.array(pc0_o3d.points)\n",
    "        pc1_o3d = o3d.geometry.PointCloud()\n",
    "        for i in range(index_in_seq1, index_in_seq1+range_of_local_graph):\n",
    "            pc1_o3d.points.extend(self.dense_scans[i])\n",
    "        pc1_o3d.voxel_down_sample(voxel_size=0.2)\n",
    "        pc1 = np.array(pc1_o3d.points)\n",
    "\n",
    "        kp0_list = []\n",
    "        kp0_for_desc = []\n",
    "        desc0_list = []\n",
    "        for i in range(index_in_seq0-range_of_global_graph, index_in_seq0):\n",
    "            kp0_list.append(self.keypoints[i])\n",
    "            for kp_idx in range(self.keypoints[i].shape[0]):\n",
    "                kp0_for_desc.append(self.keypoints[i][kp_idx])\n",
    "                desc0_list.append(self.descriptors[i][kp_idx])\n",
    "        o3d_kp0 = o3d.geometry.PointCloud()\n",
    "        for i in range(len(kp0_list)):\n",
    "            o3d_kp0.points.extend(kp0_list[i])\n",
    "\n",
    "        labels = np.array(o3d_kp0.cluster_dbscan(eps=0.3, min_points=4, print_progress=False))\n",
    "        if len(labels) > 0:\n",
    "            max_label = labels.max()  # max_label represents the number of clusters\n",
    "            merged_keypoints_chach = [[] for _ in range(max_label+1)]\n",
    "            \n",
    "            # Group keypoints based on their labels (clusters)\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label >= 0:\n",
    "                    merged_keypoints_chach[label].append(np.array(o3d_kp0.points[idx]))\n",
    "            keypoint_chach = []\n",
    "            # Calculate the mean point for each cluster and add it to keypoint_chach\n",
    "            for pi in merged_keypoints_chach:\n",
    "                p = np.mean(pi, axis=0)\n",
    "                keypoint_chach.append(p)\n",
    "        kp0 = np.array(keypoint_chach)\n",
    "\n",
    "        kp1_list = []\n",
    "        kp1_for_desc = []\n",
    "        desc1_list = []\n",
    "        for i in range(index_in_seq1, index_in_seq1 + range_of_local_graph):\n",
    "            kp1_list.append(self.keypoints[i])\n",
    "            for kp_idx in range(self.keypoints[i].shape[0]):\n",
    "                kp1_for_desc.append(self.keypoints[i][kp_idx])\n",
    "                desc1_list.append(self.descriptors[i][kp_idx])\n",
    "        o3d_kp1 = o3d.geometry.PointCloud()\n",
    "        for i in range(len(kp1_list)):\n",
    "            o3d_kp1.points.extend(kp1_list[i])\n",
    "\n",
    "        labels = np.array(o3d_kp1.cluster_dbscan(eps=0.3, min_points=4, print_progress=False))\n",
    "        if len(labels) > 0:\n",
    "            max_label = labels.max()  # max_label represents the number of clusters\n",
    "            merged_keypoints_chach = [[] for _ in range(max_label+1)]\n",
    "            \n",
    "            # Group keypoints based on their labels (clusters)\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label >= 0:\n",
    "                    merged_keypoints_chach[label].append(np.array(o3d_kp1.points[idx]))\n",
    "            keypoint_chach = []\n",
    "            # Calculate the mean point for each cluster and add it to keypoint_chach\n",
    "            for pi in merged_keypoints_chach:\n",
    "                p = np.mean(pi, axis=0)\n",
    "                keypoint_chach.append(p)\n",
    "        kp1 = np.array(keypoint_chach)\n",
    "\n",
    "        kp0_np = np.array([(kp[0], kp[1], kp[2], 1) for kp in kp0])\n",
    "        kp1_np = np.array([(kp[0], kp[1], kp[2], 1) for kp in kp1])\n",
    "        kp0_tensor = torch.tensor(kp0_np, dtype=torch.double)\n",
    "        kp1_tensor = torch.tensor(kp1_np, dtype=torch.double)\n",
    "        kp0_num = len(kp0_tensor)\n",
    "        kp1_num = len(kp1_tensor)\n",
    "\n",
    "        desc0 = []\n",
    "        scores0 = []\n",
    "        desc1 = []\n",
    "        scores1 = []\n",
    "        for i in range(kp0.shape[0]):\n",
    "            min_dist = 100000\n",
    "            min_idx = 0\n",
    "            for j in range(len(kp0_for_desc)):\n",
    "                dist = np.linalg.norm(kp0[i] - kp0_for_desc[j])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_idx = j\n",
    "            kp0[i] = kp0_for_desc[min_idx]\n",
    "            desc0.append(desc0_list[min_idx])\n",
    "            scores0.append(1)\n",
    "        desc0 = np.array(desc0)\n",
    "        scores0 = np.array(scores0)\n",
    "\n",
    "        for i in range(kp1.shape[0]):\n",
    "            min_dist = 100000\n",
    "            min_idx = 0\n",
    "            for j in range(len(kp1_for_desc)):\n",
    "                dist = np.linalg.norm(kp1[i] - kp1_for_desc[j])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_idx = j\n",
    "            kp1[i] = kp1_for_desc[min_idx]\n",
    "            desc1.append(desc1_list[min_idx])\n",
    "            scores1.append(1)\n",
    "        desc1 = np.array(desc1)\n",
    "        scores1 = np.array(scores1)\n",
    "\n",
    "        kp0_local_tensor = torch.einsum('ij,nj->ni', torch.inverse(pose0), kp0_tensor).double()\n",
    "        kp1_local_tensor = torch.einsum('ij,nj->ni', torch.inverse(pose1), kp1_tensor).double()\n",
    "\n",
    "        norm0, norm1 = np.linalg.norm(desc0, axis=1), np.linalg.norm(desc1, axis=1)\n",
    "        norm0, norm1 = norm0.reshape(kp0_num, 1), norm1.reshape(kp1_num, 1)\n",
    "        epsilon = 1e-8  # small constant to prevent division by zero\n",
    "        norm0, norm1 = norm0 + epsilon, norm1 + epsilon\n",
    "        desc0, desc1 = np.where(norm0 != 0, np.multiply(desc0, 1/norm0), 0), np.where(norm1 != 0, np.multiply(desc1, 1/norm1), 0)\n",
    "\n",
    "        desc0_tensor, desc1_tensor = torch.tensor(desc0, dtype=torch.double), torch.tensor(desc1, dtype=torch.double)\n",
    "        scores0_tensor, scores1_tensor = torch.tensor(scores0, dtype=torch.double), torch.tensor(scores1, dtype=torch.double)\n",
    "\n",
    "        dists = cdist(kp0_tensor, kp1_tensor)\n",
    "        '''Find ground true keypoint matching'''\n",
    "        min1 = np.argmin(dists, axis=0)\n",
    "        min2 = np.argmin(dists, axis=1)\n",
    "        min1v = np.min(dists, axis=1)\n",
    "        min1f = min2[min1v < 0.5]\n",
    "\n",
    "        '''For calculating repeatibility'''\n",
    "        rep = len(min1f)\n",
    "\n",
    "        match1, match2 = -1 * np.ones((len(kp0_tensor)), dtype=np.int16), -1 * np.ones((len(kp1_tensor)), dtype=np.int16)\n",
    "        match1[min1v < 0.5] = min1f\n",
    "        min2v = np.min(dists, axis=0)\n",
    "        min2f = min1[min2v < 0.5]\n",
    "        match2[min2v < 0.5] = min2f\n",
    "            \n",
    "        # print(kp0_tensor.shape)\n",
    "        # print(kp0_tensor[:,:3].shape)\n",
    "\n",
    "        return{\n",
    "            # 'skip': False,\n",
    "            'keypoints0': kp0_local_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints1': kp1_local_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints_global_0': kp0_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints_global_1': kp1_tensor[:,:3].unsqueeze(0),\n",
    "            'descriptors0': desc0_tensor.unsqueeze(0),\n",
    "            'descriptors1': desc1_tensor.unsqueeze(0),\n",
    "            'scores0': scores0_tensor.unsqueeze(0),\n",
    "            'scores1': scores1_tensor.unsqueeze(0),\n",
    "            'gt_matches0': match1,\n",
    "            'gt_matches1': match2,\n",
    "            'sequence': self.seq,\n",
    "            'idx0': index_in_seq0,\n",
    "            'idx1': index_in_seq1,\n",
    "            'pose1': pose0,\n",
    "            'pose2': pose1,\n",
    "            # 'T_cam0_velo': T_cam0_velo,\n",
    "            'T_gt': T_gt,\n",
    "            'cloud0': pc0,\n",
    "            'cloud1': pc1,\n",
    "            # 'all_matches': list(all_matches),\n",
    "            # 'file_name': file_name\n",
    "            'rep': rep\n",
    "        }\n",
    "    \n",
    "    def global_matching(self, idx):\n",
    "        pass\n",
    "                                     \n",
    "    def __len__(self):\n",
    "        return len(self.poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emplimentation 테스트 시나리오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] 1530's poses SLAM data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MDGAT(\n",
       "    (kenc): KeypointEncoder(\n",
       "      (encoder): Sequential(\n",
       "        (0): Conv1d(3, 16, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "        (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "        (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU()\n",
       "        (9): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (denc): DescriptorEncoder(\n",
       "      (encoder): Sequential(\n",
       "        (0): Conv1d(135, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "        (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (gnn): AttentionalGNN(\n",
       "      (layers): ModuleList(\n",
       "        (0): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (3): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (4): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (5): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (6): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (7): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (8): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (9): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (10): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (11): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (12): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (13): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (14): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (15): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (16): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (17): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_proj): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 데이터 로드\n",
    "dataset = Kitti_Rops_dataset_loader(opt)\n",
    "\n",
    "# 2. 모델 로드\n",
    "from models.mdgat_Rops2 import MDGAT\n",
    "path_checkpoint = opt.resume_model\n",
    "checkpoint = torch.load(path_checkpoint, map_location={'cuda:2':'cuda:0'})\n",
    "lr = checkpoint['lr_schedule']\n",
    "config = {\n",
    "        'net': {\n",
    "            'sinkhorn_iterations': opt.sinkhorn_iterations,\n",
    "            'match_threshold': opt.match_threshold,\n",
    "            'lr': lr,\n",
    "            'loss_method': opt.loss_method,\n",
    "            'k': opt.k,\n",
    "            'descriptor': opt.descriptor,\n",
    "            'mutual_check': opt.mutual_check,\n",
    "            'triplet_loss_gamma': opt.triplet_loss_gamma,\n",
    "            'train_step':opt.train_step,\n",
    "            'L':opt.l,\n",
    "            'scheduler_gamma': 0.1**(1/100)\n",
    "        }\n",
    "    }\n",
    "net = MDGAT(config.get('net', {}))\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=config.get('net', {}).get('lr'))\n",
    "net = torch.nn.DataParallel(net)\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.double().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # torch.cuda.set_device(opt.local_rank)\n",
    "    device=torch.device('cuda:{}'.format(opt.local_rank))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"### CUDA not available ###\")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매트릭 코드 짜보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx0, precision 1.000, accuracy 1.000, recall 1.000, true match 99.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx1, precision 1.000, accuracy 1.000, recall 1.000, true match 85.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx2, precision 1.000, accuracy 1.000, recall 1.000, true match 63.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx3, precision 0.968, accuracy 0.877, recall 0.843, true match 91.000, false match 0.000, fp_rate 0.000, tp_rate 0.870\n",
      "idx4, precision 1.000, accuracy 0.993, recall 0.989, true match 88.000, false match 0.000, fp_rate 0.000, tp_rate 0.989\n",
      "idx5, precision 1.000, accuracy 1.000, recall 1.000, true match 64.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx6, precision 1.000, accuracy 0.979, recall 0.974, true match 111.000, false match 0.000, fp_rate 0.000, tp_rate 0.974\n",
      "idx7, precision 1.000, accuracy 1.000, recall 1.000, true match 85.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx8, precision 1.000, accuracy 1.000, recall 1.000, true match 72.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx9, precision 1.000, accuracy 1.000, recall 1.000, true match 106.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx10, precision 0.989, accuracy 0.993, recall 0.989, true match 89.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx11, precision 0.964, accuracy 0.971, recall 0.964, true match 54.000, false match 2.000, fp_rate 0.024, tp_rate 0.964\n",
      "idx12, precision 0.957, accuracy 0.969, recall 0.978, true match 88.000, false match 2.000, fp_rate 0.050, tp_rate 1.000\n",
      "idx13, precision 0.988, accuracy 0.977, recall 0.976, true match 80.000, false match 1.000, fp_rate 0.021, tp_rate 0.976\n",
      "idx14, precision 1.000, accuracy 1.000, recall 1.000, true match 64.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx15, precision 0.991, accuracy 0.980, recall 0.974, true match 113.000, false match 0.000, fp_rate 0.000, tp_rate 0.983\n",
      "idx16, precision 1.000, accuracy 0.987, recall 0.979, true match 94.000, false match 0.000, fp_rate 0.000, tp_rate 0.979\n",
      "idx17, precision 1.000, accuracy 0.993, recall 0.986, true match 72.000, false match 0.000, fp_rate 0.000, tp_rate 0.986\n",
      "idx18, precision 1.000, accuracy 1.000, recall 1.000, true match 102.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx19, precision 0.978, accuracy 0.986, recall 0.989, true match 89.000, false match 1.000, fp_rate 0.017, tp_rate 1.000\n",
      "idx20, precision 1.000, accuracy 0.980, recall 0.957, true match 66.000, false match 0.000, fp_rate 0.000, tp_rate 0.957\n",
      "idx21, precision 1.000, accuracy 1.000, recall 1.000, true match 112.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx22, precision 0.989, accuracy 0.993, recall 1.000, true match 91.000, false match 1.000, fp_rate 0.019, tp_rate 1.000\n",
      "idx23, precision 1.000, accuracy 1.000, recall 1.000, true match 70.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx24, precision 1.000, accuracy 1.000, recall 1.000, true match 105.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx25, precision 1.000, accuracy 1.000, recall 1.000, true match 85.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx26, precision 0.986, accuracy 0.993, recall 0.986, true match 68.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx27, precision 0.991, accuracy 0.993, recall 0.991, true match 115.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx28, precision 1.000, accuracy 0.993, recall 0.989, true match 94.000, false match 0.000, fp_rate 0.000, tp_rate 0.989\n",
      "idx29, precision 0.932, accuracy 0.967, recall 0.958, true match 69.000, false match 2.000, fp_rate 0.026, tp_rate 1.000\n",
      "idx30, precision 0.992, accuracy 0.994, recall 0.992, true match 128.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx31, precision 0.990, accuracy 0.988, recall 0.981, true match 102.000, false match 0.000, fp_rate 0.000, tp_rate 0.990\n",
      "idx32, precision 0.988, accuracy 0.994, recall 0.988, true match 80.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx33, precision 1.000, accuracy 1.000, recall 1.000, true match 118.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx34, precision 0.968, accuracy 0.981, recall 0.978, true match 91.000, false match 1.000, fp_rate 0.016, tp_rate 1.000\n",
      "idx35, precision 1.000, accuracy 1.000, recall 1.000, true match 65.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx36, precision 0.984, accuracy 0.987, recall 0.984, true match 123.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx37, precision 0.970, accuracy 0.981, recall 0.980, true match 98.000, false match 1.000, fp_rate 0.017, tp_rate 1.000\n",
      "idx38, precision 0.987, accuracy 0.994, recall 1.000, true match 75.000, false match 1.000, fp_rate 0.012, tp_rate 1.000\n",
      "idx39, precision 0.992, accuracy 0.994, recall 0.992, true match 122.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx40, precision 0.990, accuracy 0.994, recall 0.990, true match 100.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx41, precision 0.974, accuracy 0.975, recall 0.962, true match 76.000, false match 1.000, fp_rate 0.012, tp_rate 0.975\n",
      "idx42, precision 0.992, accuracy 0.994, recall 1.000, true match 123.000, false match 1.000, fp_rate 0.032, tp_rate 1.000\n",
      "idx43, precision 0.990, accuracy 0.994, recall 0.990, true match 98.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx44, precision 0.954, accuracy 0.948, recall 0.912, true match 62.000, false match 2.000, fp_rate 0.023, tp_rate 0.926\n",
      "idx45, precision 0.959, accuracy 0.970, recall 0.959, true match 116.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx46, precision 0.951, accuracy 0.963, recall 0.942, true match 98.000, false match 0.000, fp_rate 0.000, tp_rate 0.990\n",
      "idx47, precision 0.947, accuracy 0.976, recall 0.947, true match 71.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx48, precision 0.976, accuracy 0.981, recall 0.992, true match 123.000, false match 2.000, fp_rate 0.054, tp_rate 1.000\n",
      "idx49, precision 0.957, accuracy 0.975, recall 0.978, true match 88.000, false match 2.000, fp_rate 0.028, tp_rate 1.000\n",
      "idx50, precision 0.973, accuracy 0.988, recall 0.986, true match 71.000, false match 1.000, fp_rate 0.011, tp_rate 1.000\n",
      "idx51, precision 0.977, accuracy 0.982, recall 0.977, true match 126.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx52, precision 0.981, accuracy 0.976, recall 0.963, true match 104.000, false match 0.000, fp_rate 0.000, tp_rate 0.981\n",
      "idx53, precision 0.959, accuracy 0.970, recall 0.946, true match 70.000, false match 1.000, fp_rate 0.011, tp_rate 0.973\n",
      "idx54, precision 1.000, accuracy 1.000, recall 1.000, true match 128.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx55, precision 0.991, accuracy 0.994, recall 0.991, true match 106.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx56, precision 0.987, accuracy 0.994, recall 0.987, true match 76.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx57, precision 0.971, accuracy 0.973, recall 0.971, true match 134.000, false match 1.000, fp_rate 0.022, tp_rate 0.993\n",
      "idx58, precision 0.991, accuracy 0.995, recall 0.991, true match 115.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx59, precision 0.989, accuracy 0.989, recall 0.989, true match 86.000, false match 1.000, fp_rate 0.010, tp_rate 0.989\n",
      "idx60, precision 0.986, accuracy 0.989, recall 0.993, true match 136.000, false match 1.000, fp_rate 0.024, tp_rate 1.000\n",
      "idx61, precision 1.000, accuracy 1.000, recall 1.000, true match 115.000, false match 0.000, fp_rate 0.000, tp_rate 1.000\n",
      "idx62, precision 0.976, accuracy 0.983, recall 0.976, true match 82.000, false match 1.000, fp_rate 0.011, tp_rate 0.988\n",
      "idx63, precision 0.972, accuracy 0.978, recall 0.979, true match 139.000, false match 1.000, fp_rate 0.027, tp_rate 1.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(pred[p]) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     12\u001b[0m         pred[p] \u001b[38;5;241m=\u001b[39m pred[p]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_mdgat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m pred \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata}\n\u001b[1;32m     16\u001b[0m kpts0, kpts1 \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/ADD_prj/MDGAT-matcher/models/mdgat_Rops2.py:396\u001b[0m, in \u001b[0;36mMDGAT.infer_mdgat\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    393\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbdn,bdm->bnm\u001b[39m\u001b[38;5;124m'\u001b[39m, mdesc0, mdesc1)\n\u001b[1;32m    394\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptor_dim\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m.5\u001b[39m\n\u001b[0;32m--> 396\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mlog_optimal_transport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbin_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m\u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msinkhorn_iterations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''directly determine the non_matches on scores matrix'''\u001b[39;00m\n\u001b[1;32m    401\u001b[0m max0, max1 \u001b[38;5;241m=\u001b[39m scores[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m2\u001b[39m), scores[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/ADD_prj/MDGAT-matcher/models/mdgat_Rops2.py:298\u001b[0m, in \u001b[0;36mlog_optimal_transport\u001b[0;34m(scores, alpha, iters)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Perform Differentiable Optimal Transport in Log-space for stability\"\"\"\u001b[39;00m\n\u001b[1;32m    297\u001b[0m b, m, n \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 298\u001b[0m one \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m ms, ns \u001b[38;5;241m=\u001b[39m (m\u001b[38;5;241m*\u001b[39mone)\u001b[38;5;241m.\u001b[39mto(scores), (n\u001b[38;5;241m*\u001b[39mone)\u001b[38;5;241m.\u001b[39mto(scores)\n\u001b[1;32m    301\u001b[0m bins0 \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m.\u001b[39mexpand(b, m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_test_loss = []; precision_array = []; accuracy_array = []; recall_array = []\n",
    "trans_error_array = []; rot_error_array = []; relative_trans_error_array = []; relative_rot_error_array = []\n",
    "repeatibilty_array = []; valid_num_array = []; all_num_array = []; inlier_array = [] \n",
    "kpnum_array = []; fp_rate_array = []; tp_rate_array = []; tp_rate2_array = []; inlier_ratio_array= [];tm_a=[];fm_a=[]\n",
    "fail = 0\n",
    "baned_data = 0\n",
    "for pair in range(len(dataset.gt_pairs)):\n",
    "# for pair in [0]:\n",
    "    pred = dataset.get_gt_pairs(pair)\n",
    "    for p in pred:\n",
    "        if type(pred[p]) == torch.Tensor:\n",
    "            pred[p] = pred[p].to(device)\n",
    "    data = net.module.infer_mdgat(pred)\n",
    "    pred = {**pred, **data}\n",
    "\n",
    "    kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "    kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "    matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "    gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "    valid = matches0 > -1\n",
    "    mkpts0 = kpts0[valid]\n",
    "    mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "    mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "    mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "    mutual1 = matches0[mutual0]\n",
    "    x = np.ones(len(matches1)) == 1\n",
    "    x[mutual1] = False\n",
    "    valid1 = matches1 > -1\n",
    "\n",
    "    mconf = conf[valid]\n",
    "\n",
    "    ## ground truth ##\n",
    "    matches_gt, matches_gt1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "    matches_gt[matches_gt == len(matches_gt1)] = -1\n",
    "    matches_gt1[matches_gt1 == len(matches_gt)] = -1\n",
    "    valid_gt = matches_gt > -1\n",
    "\n",
    "    valid_num = np.sum(valid_gt)\n",
    "    all_num = len(valid_gt)\n",
    "    repeatibilty = valid_num/all_num\n",
    "    repeatibilty_array.append(repeatibilty)\n",
    "\n",
    "    mkpts0_gt = kpts0[valid_gt]\n",
    "    mkpts1_gt = kpts1[matches_gt[valid_gt]]\n",
    "    mutual0 = np.arange(len(matches_gt))[valid_gt] == matches_gt1[matches_gt[valid_gt]]\n",
    "    # mutual0_inv = 1-mutual0\n",
    "    mutual0 = np.arange(len(matches_gt))[valid_gt][mutual0]\n",
    "    mutual1 = matches_gt[mutual0]\n",
    "    x = np.ones(len(matches_gt1)) == 1\n",
    "    x[mutual1] = False               \n",
    "    valid_gt1 = matches_gt1 > -1\n",
    "\n",
    "    mscores_gt = pred['scores0'][0].cpu().numpy()[valid_gt]\n",
    "    gt_idx = np.arange(len(kpts0))[valid_gt]\n",
    "\n",
    "    if len(mkpts0) < 4:\n",
    "        fail+=1\n",
    "        print('registration fail')\n",
    "\n",
    "    ''' calculate false positive ,true positive ,true nagetive, precision, accuracy, recall '''\n",
    "    true_positive = [(matches0[i] == matches_gt[i]) and (valid[i]) for i in range(len(kpts0))]\n",
    "    true_negativate = [(matches0[i] == matches_gt[i]) and not (valid[i]) for i in range(len(kpts0))]\n",
    "    false_positive = [valid[i] and (matches_gt[i]==-1) for i in range(len(kpts0))]\n",
    "    ckpts0 = kpts0[true_positive]\n",
    "    ckpts1 = [matches0[true_positive]]\n",
    "    precision = np.sum(true_positive) / np.sum(valid) if np.sum(valid) > 0 else 0\n",
    "    recall = np.sum(true_positive) / np.sum(valid_gt) if np.sum(valid) > 0 else 0\n",
    "    tm = np.sum(true_positive) \n",
    "    fm = np.sum(false_positive) \n",
    "    matching_score = np.sum(true_positive) / len(kpts0) if len(kpts0) > 0 else 0\n",
    "    accuracy = (np.sum(true_positive) + np.sum(true_negativate))/len(matches_gt)\n",
    "    fp_rate = np.sum(false_positive)/np.sum(matches_gt==-1)\n",
    "    tp_rate = np.sum([valid[i] and (matches_gt[i]>-1) for i in range(len(kpts0))])/np.sum(matches_gt > -1)\n",
    "    tp_rate2 = np.sum(true_positive)/np.sum(matches_gt > -1)\n",
    "    T=[]\n",
    "    print('idx{}, precision {:.3f}, accuracy {:.3f}, recall {:.3f}, true match {:.3f}, false match {:.3f}, fp_rate {:.3f}, tp_rate {:.3f}'.format(\n",
    "        pair, precision, accuracy, recall,tm,fm, fp_rate, tp_rate))\n",
    "    precision_array.append(precision)\n",
    "    accuracy_array.append(accuracy)\n",
    "    recall_array.append(recall)\n",
    "    fp_rate_array.append(fp_rate)\n",
    "    tp_rate_array.append(tp_rate)\n",
    "    tp_rate2_array.append(tp_rate2)\n",
    "    tm_a.append(tm)\n",
    "    fm_a.append(fm)\n",
    "precision_mean = np.mean(precision_array)\n",
    "accuracy_mean = np.mean(accuracy_array)\n",
    "recall_mean = np.mean(recall_array)\n",
    "repeatibilty_array_mean = np.mean(repeatibilty_array)\n",
    "fp_rate_mean = np.mean(fp_rate_array)\n",
    "tp_rate_mean = np.mean(tp_rate_array)\n",
    "tp_rate_mean2 = np.mean(tp_rate2_array)\n",
    "tm = np.mean(tm_a)\n",
    "fm = np.mean(fm_a)\n",
    "print('average repeatibility: {:.3f}, fail {:.6f}, precision_mean {:.3f}, accuracy_mean {:.3f}, recall_mean {:.3f}, true match {:.3f}, false match {:.3f}, fp_rate_mean {:.3f}, tp_rate_mean {:.3f}, tp_rate_mean2 {:.3f}, trans_error_mean {:.3f}, rot_error_mean {:.3f} '.format(\n",
    "    repeatibilty_array_mean, fail/pair, precision_mean, accuracy_mean, recall_mean,tm,fm, fp_rate_mean, tp_rate_mean, tp_rate_mean2, trans_error_mean, rot_error_mean ))\n",
    "# print('valid num {}, all_num {}'.format(valid_num_mean, all_num_mean))\n",
    "print('baned_data {}'.format(baned_data/pair))\n",
    "\n",
    "  #average repeatibility: 0.593, fail 0.002518, precision_mean 0.947, accuracy_mean 0.944, recall_mean 0.915, true match 111.825, false match 2.146, fp_rate_mean 0.027, tp_rate_mean 0.939, tp_rate_mean2 0.915, trans_error_mean nan, rot_error_mean nan \n",
    "# baned_data 0.0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매칭 비주얼로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m line_set\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(colors)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpcd_kp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd_kp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_set\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for pair in range(0, len(dataset.gt_pairs), 10):\n",
    "    pred = dataset.get_gt_pairs(pair)\n",
    "    for p in pred:\n",
    "        if type(pred[p]) == torch.Tensor:\n",
    "            pred[p] = pred[p].to(device)\n",
    "    data = net.module.infer_mdgat(pred)\n",
    "    pred = {**pred, **data}\n",
    "\n",
    "    kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "    kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "    matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "    gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "    valid = matches0 > -1\n",
    "    mkpts0 = kpts0[valid]\n",
    "    mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "    mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "    mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "    mutual1 = matches0[mutual0]\n",
    "    x = np.ones(len(matches1)) == 1\n",
    "    x[mutual1] = False\n",
    "    valid1 = matches1 > -1\n",
    "\n",
    "    mconf = conf[valid]    \n",
    "\n",
    "    pcd_kp0 = o3d.geometry.PointCloud()\n",
    "    pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "    pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "    pcd_kp1 = o3d.geometry.PointCloud()\n",
    "    pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "    pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "    points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "    lines = []\n",
    "    colors = []\n",
    "    for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "        lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "        # lines.append([match, mutual1[idx] + len(kpts_g_0)])\n",
    "        point1 = kpts_g_0[match]\n",
    "        point2 = kpts_g_1[mutual1[idx]]\n",
    "        if np.linalg.norm(point1 - point2) < 1.0:\n",
    "            colors.append([0, 1, 0])\n",
    "        else: \n",
    "            colors.append([1, 0, 0])\n",
    "    line_set = o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector((points)),\n",
    "        lines=o3d.utility.Vector2iVector(lines),\n",
    "    )\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    # o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "    # o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n",
    "    o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제상황처럼 테스트 해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] 1530's poses SLAM data loaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Kitti_Rops_dataset_loader(opt)\n\u001b[0;32m----> 3\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatching_test_1to2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m kp0 \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints_global_0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      6\u001b[0m kp1 \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints_global_1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[3], line 264\u001b[0m, in \u001b[0;36mKitti_Rops_dataset_loader.matching_test_1to2\u001b[0;34m(self, idx, range_of_global_graph, range_of_local_graph)\u001b[0m\n\u001b[1;32m    262\u001b[0m min_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(kp1_for_desc)):\n\u001b[0;32m--> 264\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkp1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkp1_for_desc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;241m<\u001b[39m min_dist:\n\u001b[1;32m    266\u001b[0m         min_dist \u001b[38;5;241m=\u001b[39m dist\n",
      "File \u001b[0;32m~/anaconda3/envs/mdgat/lib/python3.10/site-packages/numpy/linalg/linalg.py:2546\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2541\u001b[0m ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m     (\u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m     (\u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m-> 2546\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isComplexType(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype):\n\u001b[1;32m   2548\u001b[0m         x_real \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreal\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Kitti_Rops_dataset_loader(opt)\n",
    "\n",
    "pred = dataset.matching_test_1to2(500, 50, 50)\n",
    "\n",
    "kp0 = pred['keypoints_global_0'][0].cpu().numpy()\n",
    "kp1 = pred['keypoints_global_1'][0].cpu().numpy()\n",
    "pc0 = pred['cloud0']\n",
    "pc1 = pred['cloud1']\n",
    "\n",
    "kp0_o3d = o3d.geometry.PointCloud()\n",
    "kp0_o3d.points = o3d.utility.Vector3dVector(kp0)\n",
    "kp0_o3d.paint_uniform_color([0, 1, 0])\n",
    "kp1_o3d = o3d.geometry.PointCloud()\n",
    "kp1_o3d.points = o3d.utility.Vector3dVector(kp1)\n",
    "kp1_o3d.paint_uniform_color([1, 0, 0])\n",
    "pc0_o3d = o3d.geometry.PointCloud()\n",
    "pc0_o3d.points = o3d.utility.Vector3dVector(pc0)\n",
    "pc0_o3d.paint_uniform_color([0.7, 1, 0.7])\n",
    "pc1_o3d = o3d.geometry.PointCloud()\n",
    "pc1_o3d.points = o3d.utility.Vector3dVector(pc1)\n",
    "pc1_o3d.paint_uniform_color([1, 0.7, 0.7])\n",
    "\n",
    "# o3d.visualization.draw_geometries([kp0_o3d, kp1_o3d, pc0_o3d, pc1_o3d])\n",
    "o3d.visualization.draw_geometries([kp0_o3d, kp1_o3d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 131, 3]) torch.Size([1, 158, 3]) torch.Size([1, 131, 135]) torch.Size([1, 158, 135]) torch.Size([1, 131]) torch.Size([1, 158])\n",
      "torch.Size([1, 131, 3]) torch.Size([1, 249, 3]) torch.Size([1, 131, 135]) torch.Size([1, 249, 135]) torch.Size([1, 131]) torch.Size([1, 249])\n",
      "torch.Size([1, 131, 3]) torch.Size([1, 343, 3]) torch.Size([1, 131, 135]) torch.Size([1, 343, 135]) torch.Size([1, 131]) torch.Size([1, 343])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m line_set\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(colors)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpcd_kp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd_kp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_set\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for pair in range(160, len(dataset.gt_pairs), 10):\n",
    "pair = 140\n",
    "for range0 in [10, 20, 30, 40, 50, 60]:\n",
    "    for range1 in [10, 20, 30, 40, 50, 60]:\n",
    "        pred = dataset.matching_test_1to2(pair, range0, range1)\n",
    "        for p in pred:\n",
    "            if type(pred[p]) == torch.Tensor:\n",
    "                pred[p] = pred[p].to(device)\n",
    "        print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape, pred['scores0'].shape, pred['scores1'].shape)\n",
    "\n",
    "        data = net.module.infer_mdgat(pred)\n",
    "        pred = {**pred, **data}\n",
    "\n",
    "        kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "        kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "        matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "        gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "        valid = matches0 > -1\n",
    "        mkpts0 = kpts0[valid]\n",
    "        mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "        mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "        mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "        mutual1 = matches0[mutual0]\n",
    "        x = np.ones(len(matches1)) == 1\n",
    "        x[mutual1] = False\n",
    "        valid1 = matches1 > -1\n",
    "\n",
    "        mconf = conf[valid]    \n",
    "\n",
    "        pcd_kp0 = o3d.geometry.PointCloud()\n",
    "        pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "        pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "        pcd_kp1 = o3d.geometry.PointCloud()\n",
    "        pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "        pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "        points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "        lines = []\n",
    "        colors = []\n",
    "        for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "            lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "            # lines.append([match, mutual1[idx] + len(kpts_g_0)])\n",
    "            point1 = kpts_g_0[match]\n",
    "            point2 = kpts_g_1[mutual1[idx]]\n",
    "            if np.linalg.norm(point1 - point2) < 1.0:\n",
    "                colors.append([0, 1, 0])\n",
    "            else: \n",
    "                colors.append([1, 0, 0])\n",
    "        line_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector((points)),\n",
    "            lines=o3d.utility.Vector2iVector(lines),\n",
    "        )\n",
    "        line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "        # o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "        # o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n",
    "        o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in range(160, len(dataset.gt_pairs), 10):\n",
    "pair = 140\n",
    "for range0 in [10, 20, 30, 40, 50, 60]:\n",
    "    for range1 in [10, 20, 30, 40, 50, 60]:\n",
    "        pred = dataset.matching_test_1to2(pair, range0, range1)\n",
    "        for p in pred:\n",
    "            if type(pred[p]) == torch.Tensor:\n",
    "                pred[p] = pred[p].to(device)\n",
    "        print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape, pred['scores0'].shape, pred['scores1'].shape)\n",
    "\n",
    "        data = net.module.infer_mdgat(pred)\n",
    "        pred = {**pred, **data}\n",
    "\n",
    "        kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "        kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "        matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "        gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "        valid = matches0 > -1\n",
    "        mkpts0 = kpts0[valid]\n",
    "        mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "        mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "        mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "        mutual1 = matches0[mutual0]\n",
    "        x = np.ones(len(matches1)) == 1\n",
    "        x[mutual1] = False\n",
    "        valid1 = matches1 > -1\n",
    "\n",
    "        mconf = conf[valid]\n",
    "\n",
    "        gt_match_num = 0\n",
    "        for i in gt_match0:\n",
    "            if i > -1:\n",
    "                gt_match_num+=1\n",
    "                \n",
    "        matched_num = 0\n",
    "        miss_matched_num = 0\n",
    "        for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "            point1 = kpts_g_0[match]\n",
    "            point2 = kpts_g_1[mutual1[idx]]\n",
    "            if np.linalg.norm(point1 - point2) < 0.5:\n",
    "                matched_num+=1\n",
    "            else: \n",
    "                miss_matched_num+=1\n",
    "\n",
    "        print('range0 {}, range1 {}, gt_match_num {}, matched_num {}, miss_matched_num {}'.format(range0, range1, gt_match_num, matched_num, miss_matched_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "valid = matches0 > -1\n",
    "mkpts0 = kpts0[valid]\n",
    "mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "mutual1 = matches0[mutual0]\n",
    "x = np.ones(len(matches1)) == 1\n",
    "x[mutual1] = False\n",
    "valid1 = matches1 > -1\n",
    "\n",
    "mconf = conf[valid]\n",
    "\n",
    "# matches_gt, matches_gt1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "# matches_gt[matches_gt == len(matches_gt1)] = -1\n",
    "# matches_gt1[matches_gt1 == len(matches_gt)] = -1\n",
    "# valid_gt = matches_gt > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mutual0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_kp_g_0 = o3d.geometry.PointCloud()\n",
    "pcd_kp_g_0.points = o3d.utility.Vector3dVector(kpts_g_0)\n",
    "pcd_kp_g_0.paint_uniform_color([0, 0, 1])\n",
    "pcd_kp_g_1 = o3d.geometry.PointCloud()\n",
    "pcd_kp_g_1.points = o3d.utility.Vector3dVector(kpts_g_1)\n",
    "pcd_kp_g_1.paint_uniform_color([0, 1, 0])\n",
    "o3d.visualization.draw_geometries([pcd_kp_g_0, pcd_kp_g_1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "pcd_kp_g_0 = o3d.geometry.PointCloud()\n",
    "pcd_kp_g_0.points = o3d.utility.Vector3dVector(kpts_g_0)\n",
    "pcd_kp_g_0.paint_uniform_color([0, 0, 1])\n",
    "pcd_kp_g_1 = o3d.geometry.PointCloud()\n",
    "pcd_kp_g_1.points = o3d.utility.Vector3dVector(kpts_g_1)\n",
    "pcd_kp_g_1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "points = np.concatenate((np.array(pcd_kp_g_0.points),np.array(pcd_kp_g_1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "lines = []\n",
    "colors = []\n",
    "for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "    lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "    # lines.append([match, mutual1[idx] + len(kpts_g_0)])\n",
    "    point1 = kpts_g_0[match]\n",
    "    point2 = kpts_g_1[mutual1[idx]]\n",
    "    if np.linalg.norm(point1 - point2) < 1.0:\n",
    "        colors.append([0, 1, 0])\n",
    "    else: \n",
    "        colors.append([1, 0, 0])\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector((points)),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "# o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n",
    "o3d.visualization.draw_geometries([pcd_kp_g_0, pcd_kp_g_1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kpts0.shape, kpts_g_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dataset.get_divided_data(2)\n",
    "print(pred['keypoints0'].shape, pred['cloud0'].shape, pred['scores0'].shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매칭\n",
    "# pred = dataset.get_matching_data(12)\n",
    "# pred = dataset.get_matching_data(13)\n",
    "# pred = dataset.get_matching_data(12)\n",
    "for p in pred:\n",
    "    if type(pred[p]) == torch.Tensor:\n",
    "        pred[p] = pred[p].to(device)\n",
    "print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape)\n",
    "\n",
    "data = net.module.infer_mdgat(pred, [pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//4, None, pred['keypoints0'].shape[1]//4, None], [pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//4, None, pred['keypoints1'].shape[1]//4, None])\n",
    "pred = {**pred, **data}\n",
    "print(\"pred's keys: \", pred.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 디스크립터 추출\n",
    "net.double().eval()\n",
    "\n",
    "for i in range(dataset.divided_seq_num - 1):\n",
    "    pred = dataset.get_divided_data(i)\n",
    "    for p in pred:\n",
    "        pred[p] = pred[p].to(device)\n",
    "    data = net.module.infer_desc(pred)\n",
    "    for d in data:\n",
    "        data[d] = data[d].detach().cpu()\n",
    "    dataset.push_descriptor(i, data['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 매칭\n",
    "# pred = dataset.get_matching_data(12)\n",
    "pred = dataset.get_matching_data(13)\n",
    "# pred = dataset.get_matching_data(12)\n",
    "for p in pred:\n",
    "    if type(pred[p]) == torch.Tensor:\n",
    "        pred[p] = pred[p].to(device)\n",
    "print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape)\n",
    "\n",
    "data = net.module.infer_mdgat(pred, [pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//4, None, pred['keypoints0'].shape[1]//4, None], [pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//4, None, pred['keypoints1'].shape[1]//4, None])\n",
    "pred = {**pred, **data}\n",
    "print(\"pred's keys: \", pred.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "valid = matches0 > -1\n",
    "mkpts0 = kpts0[valid]\n",
    "mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "mutual1 = matches0[mutual0]\n",
    "x = np.ones(len(matches1)) == 1\n",
    "x[mutual1] = False\n",
    "valid1 = matches1 > -1\n",
    "\n",
    "mconf = conf[valid]\n",
    "\n",
    "matches_gt, matches_gt1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "matches_gt[matches_gt == len(matches_gt1)] = -1\n",
    "matches_gt1[matches_gt1 == len(matches_gt)] = -1\n",
    "valid_gt = matches_gt > -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(matches0.shape, matches1.shape, conf.shape) # >> (3328,) (256,) (3328,)\n",
    "\n",
    "print(matches0[669]) # matches0의 669번째 인덱스와의 매칭 결과 >> 1\n",
    "\n",
    "print(matches1[:]) # matches1의 전체 결과 >> [  -1  669   -1  677 ...], 즉 matches1의 1번째 포인트가 matches0의 669번째 포인트와 매칭됨\n",
    "\n",
    "print(mutual0) # >> [   9   10   13   15 ...]\n",
    "\n",
    "print(mutual1) # >> [248 245  20 255 106 ...]\n",
    "\n",
    "print(matches0[9], matches1[248]) # >> 248, 9, 즉 mutual0[i]와 mutual1[i]의 idx끼리 매칭 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "lines = []\n",
    "for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "    # lines.append([match, match + 1])\n",
    "    # lines.append([mutual1[idx] + len(kpts0), mutual1[idx] + len(kpts0)])\n",
    "    lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "colors = [[0, 1, 0] for _ in range(len(lines))] # lines are shown in green\n",
    "# print(points[lines[0][0]], points[lines[0][1]])\n",
    "# print(pcd_kp0.points[lines[0][0]], pcd_kp1.points[lines[0][1] - len(pcd_kp0.points)])\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector((points)),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = 2.0\n",
    "\n",
    "# 시각화\n",
    "kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "desc0 = pred['descriptors0'][0].cpu().detach().numpy().T\n",
    "desc1 = pred['descriptors1'][0].cpu().detach().numpy().T\n",
    "dists = cdist(desc0, desc1)\n",
    "min0 = np.argmin(dists, axis=0)\n",
    "min1 = np.argmin(dists, axis=1)\n",
    "min0v = np.min(dists, axis=1)\n",
    "min0f = min1[min0v < ttt]\n",
    "match0, match1 = -1 * np.ones((len(desc0)), dtype=np.int16), -1 * np.ones((len(desc1)), dtype=np.int16)\n",
    "match0[min0v < ttt] = min0f\n",
    "min1v = np.min(dists, axis=0)\n",
    "min1f = min0[min1v < ttt]\n",
    "match1[min1v < ttt] = min1f\n",
    "mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "mutual1 = matches0[mutual0]\n",
    "\n",
    "# 시각화\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "lines = []\n",
    "for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "    lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "colors = [[0, 1, 0] for _ in range(len(lines))] # lines are shown in green\n",
    "# print(points[lines[0][0]], points[lines[0][1]])\n",
    "# print(pcd_kp0.points[lines[0][0]], pcd_kp1.points[lines[0][1] - len(pcd_kp0.points)])\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector((points)),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mutual0)\n",
    "print(mutual1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
