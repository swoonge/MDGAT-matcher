{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, sys, signal,rospy, argparse, csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.distance import cdist\n",
    "import copy, pickle\n",
    "import open3d as o3d\n",
    "import open3d.visualization as vis\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from open3d_ros_helper import open3d_ros_helper as orh\n",
    "from geometry_msgs.msg import Pose, PoseArray, Point # PoseArray, Pose\n",
    "\n",
    "from models.mdgat_denseKITTI import MDGAT\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='Point cloud matching and pose evaluation',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    dataset_dir = '/media/vision/Seagate/DataSets/denseKITTI',\n",
    "    # slam_dir = '',\n",
    "    data_folder = 'harris_3D',\n",
    "    local_global = False,\n",
    "    seq_num = 0,\n",
    "    visualize = False,\n",
    "    vis_line_width = 0.2,\n",
    "    calculate_pose = True,\n",
    "    learning_rate = 0.0001,\n",
    "    batch_size = 1,\n",
    "    train_path = './denseKITTI/',\n",
    "    model_out_path = './models/checkpoint',\n",
    "    memory_is_enough = True,\n",
    "    local_rank = 0,\n",
    "    txt_path = './KITTI/preprocess-random-full',\n",
    "    keypoints_path = './denseKITTI/keypoints',\n",
    "    resume_model = './checkpoint/denseKITTI/mdgat-l9-gap_loss-pointnet-06_07_15_02/train_step3/nomutualcheck-mdgat-batch32-lr0.0001-gap_loss-pointnet-Hariss3D-06_07_15_02/model_epoch_155.pth',\n",
    "    loss_method = 'gap_loss',\n",
    "    net = 'mdgat',\n",
    "    mutual_check = False,\n",
    "    k = [64, None, 64, None, 32, None, 32, None],\n",
    "    l = 9,\n",
    "    descriptor = 'pointnet',\n",
    "    keypoints = 'harris_3D',\n",
    "    ensure_kpts_num = False,\n",
    "    max_keypoints = -1,\n",
    "    match_threshold = 0.2,\n",
    "    threshold = 0.5,\n",
    "    triplet_loss_gamma = 0.5,\n",
    "    sinkhorn_iterations = 100,\n",
    "    train_step = 3,\n",
    "\n",
    "    fpfh_normal_radiuse = 0.3,\n",
    "    fpfh_descriptors_radiuse = 1.0,\n",
    "    seq_list = [0],\n",
    "    mdgat_path = './KITTI',\n",
    "    kitti_path = '/media/vision/Seagate/DataSets/kitti/dataset/sequences',\n",
    "    transform_opt = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kitti_Rops_dataset_loader():\n",
    "    def __init__(self, args, seq_num) -> None:\n",
    "        self.dataset_dir = args.dataset_dir\n",
    "        self.seq = seq_num\n",
    "        self.descriptor_type = args.descriptor\n",
    "\n",
    "        self.gt_pairs = []\n",
    "        self.poses = []\n",
    "        self.keypoints = []\n",
    "        self.scores = []\n",
    "        self.descriptors = []\n",
    "        self.dense_scans = []\n",
    "\n",
    "        self.local_graph_range = [0, 0]\n",
    "        self.divided_keypoints = np.array([])\n",
    "        self.divided_dense_scans = []\n",
    "        self.divided_seq_num = 0\n",
    "\n",
    "        self._load_gt_pairs()\n",
    "        self._load_datas()\n",
    "        print(\"[Load] %d's poses SLAM data loaded\" % len(self.poses))\n",
    "    \n",
    "    def _load_gt_pairs(self):\n",
    "        file_path = os.path.join(self.dataset_dir, 'groundtruths128', '%02d'%self.seq, 'groundtruths.txt')\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines_list = f.readlines()\n",
    "            for i, line_str in enumerate(lines_list):\n",
    "                if i == 0:\n",
    "                    continue # skip the header line\n",
    "                line_splitted = line_str.split()\n",
    "                anc_idx = int(float(line_splitted[0]))\n",
    "                pos_idx = int(float(line_splitted[1]))\n",
    "\n",
    "                data = {'seq': self.seq, 'anc_idx': anc_idx, 'pos_idx': pos_idx}\n",
    "                self.gt_pairs.append(data)\n",
    "\n",
    "    def _load_datas(self):\n",
    "        # poses\n",
    "        pose_path = os.path.join(self.dataset_dir, 'poses', '%02d.txt'%self.seq)\n",
    "        with open(pose_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                T_w_cam0 = np.fromstring(line, dtype=float, sep=' ')\n",
    "                T_w_cam0 = T_w_cam0.reshape(3, 4)\n",
    "                T_w_cam0 = np.vstack((T_w_cam0, [0, 0, 0, 1]))\n",
    "                self.poses.append(T_w_cam0)\n",
    "\n",
    "        # keypoints, scores and descriptors\n",
    "        keypoint_folder = os.path.join(self.dataset_dir, 'keypoints', '%02d'%self.seq)\n",
    "        keypoint_folder = os.listdir(keypoint_folder)   \n",
    "        keypoint_folder.sort(key=lambda x:int(x[:-4]))\n",
    "        for idx in range(len(keypoint_folder)):\n",
    "            file = os.path.join(self.dataset_dir, 'keypoints', '%02d'%self.seq, keypoint_folder[idx])\n",
    "            if os.path.isfile(file):\n",
    "                pc = np.reshape(np.fromfile(file, dtype=np.float64), (-1, 139))\n",
    "                self.keypoints.append(pc[:, :3])\n",
    "                self.scores.append(pc[:, 3])\n",
    "                self.descriptors.append(pc[:, 4:])\n",
    "            else:\n",
    "                self.keypoints.append([0.0, 0.0, 0.0])\n",
    "                self.scores.append([0])\n",
    "                self.descriptors.append([0.0]*135)\n",
    "\n",
    "        # dense scans\n",
    "        dense_folder = os.path.join(self.dataset_dir, 'dense_scan', '%02d'%self.seq)\n",
    "        dense_folder = os.listdir(dense_folder)\n",
    "        dense_folder.sort(key=lambda x:int(x[:-4]))\n",
    "        for idx in range(len(dense_folder)):\n",
    "            file = os.path.join(self.dataset_dir, 'dense_scan', '%02d'%self.seq, dense_folder[idx])\n",
    "            if os.path.isfile(file):\n",
    "                self.dense_scans.append(np.reshape(np.fromfile(file, dtype=np.float64), (-1, 3)))\n",
    "            else:\n",
    "                self.dense_scans.append(np.array([0, 0, 0]))\n",
    "\n",
    "    def get_gt_pairs(self, idx):\n",
    "        index_in_seq0 = self.gt_pairs[idx]['anc_idx']\n",
    "        index_in_seq1 = self.gt_pairs[idx]['pos_idx']\n",
    "\n",
    "        pose0 = torch.tensor(self.poses[index_in_seq0], dtype=torch.double)\n",
    "        pose1 = torch.tensor(self.poses[index_in_seq1], dtype=torch.double)\n",
    "        T_gt = torch.einsum('ab,de->ae', torch.inverse(pose0), pose1)\n",
    "\n",
    "        pc1_w = self.dense_scans[index_in_seq0]\n",
    "        pc2_w = self.dense_scans[index_in_seq1]\n",
    "\n",
    "        pc1_w = np.array([(kp[0], kp[1], kp[2], 1) for kp in pc1_w]) \n",
    "        pc2_w = np.array([(kp[0], kp[1], kp[2], 1) for kp in pc2_w])\n",
    "        pc1_w, pc2_w = torch.tensor(pc1_w, dtype=torch.double), torch.tensor(pc2_w, dtype=torch.double)\n",
    "\n",
    "        kp0_np = np.array([(kp[0], kp[1], kp[2], 1) for kp in self.keypoints[index_in_seq0]]) \n",
    "        kp1_np = np.array([(kp[0], kp[1], kp[2], 1) for kp in self.keypoints[index_in_seq1]])\n",
    "        kp0_tensor = torch.tensor(kp0_np, dtype=torch.double)\n",
    "        kp1_tensor = torch.tensor(kp1_np, dtype=torch.double)\n",
    "        pc1 = torch.einsum('ij,nj->ni', torch.inverse(pose0), pc1_w)\n",
    "        pc2 = torch.einsum('ij,nj->ni', torch.inverse(pose1), pc2_w)\n",
    "        kp0_local_tensor = torch.einsum('ij,nj->ni', torch.inverse(pose0), kp0_tensor).double()\n",
    "        kp1_local_tensor = torch.einsum('ij,nj->ni', torch.inverse(pose1), kp1_tensor).double()\n",
    "\n",
    "        kp0_local_tensor = kp0_local_tensor[:, :3]\n",
    "        kp1_local_tensor = kp1_local_tensor[:, :3]\n",
    "        pc1 = pc1[:, :3]\n",
    "        pc2 = pc2[:, :3]\n",
    "\n",
    "        desc0 = self.descriptors[index_in_seq0]\n",
    "        desc1 = self.descriptors[index_in_seq1]\n",
    "\n",
    "        kp0_num = len(kp0_tensor)\n",
    "        kp1_num = len(kp1_tensor)\n",
    "\n",
    "        norm0, norm1 = np.linalg.norm(desc0, axis=1), np.linalg.norm(desc1, axis=1)\n",
    "        norm0, norm1 = norm0.reshape(kp0_num, 1), norm1.reshape(kp1_num, 1)\n",
    "        epsilon = 1e-8  # small constant to prevent division by zero\n",
    "        norm0, norm1 = norm0 + epsilon, norm1 + epsilon\n",
    "        desc0, desc1 = np.where(norm0 != 0, np.multiply(desc0, 1/norm0), 0), np.where(norm1 != 0, np.multiply(desc1, 1/norm1), 0)\n",
    "\n",
    "        desc0_tensor, desc1_tensor = torch.tensor(desc0, dtype=torch.double), torch.tensor(desc1, dtype=torch.double)\n",
    "        scores0_tensor, scores1_tensor = torch.tensor(self.scores[index_in_seq0], dtype=torch.double), torch.tensor(self.scores[index_in_seq1], dtype=torch.double)\n",
    "\n",
    "        dists = cdist(kp0_tensor, kp1_tensor)\n",
    "        '''Find ground true keypoint matching'''\n",
    "        min1 = np.argmin(dists, axis=0)\n",
    "        min2 = np.argmin(dists, axis=1)\n",
    "        min1v = np.min(dists, axis=1)\n",
    "        min1f = min2[min1v < 0.5]\n",
    "\n",
    "        '''For calculating repeatibility'''\n",
    "        rep = len(min1f)\n",
    "\n",
    "        match1, match2 = -1 * np.ones((len(kp0_tensor)), dtype=np.int16), -1 * np.ones((len(kp1_tensor)), dtype=np.int16)\n",
    "        match1[min1v < 0.5] = min1f\n",
    "        min2v = np.min(dists, axis=0)\n",
    "        min2f = min1[min2v < 0.5]\n",
    "        match2[min2v < 0.5] = min2f\n",
    "            \n",
    "        # print(kp0_tensor.shape)\n",
    "        # print(kp0_tensor[:,:3].shape)\n",
    "\n",
    "        return{\n",
    "            # 'skip': False,\n",
    "            'keypoints0': kp0_local_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints1': kp1_local_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints_global_0': kp0_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints_global_1': kp1_tensor[:,:3].unsqueeze(0),\n",
    "            'descriptors0': desc0_tensor.unsqueeze(0),\n",
    "            'descriptors1': desc1_tensor.unsqueeze(0),\n",
    "            'scores0': scores0_tensor.unsqueeze(0),\n",
    "            'scores1': scores1_tensor.unsqueeze(0),\n",
    "            'gt_matches0': match1,\n",
    "            'gt_matches1': match2,\n",
    "            'sequence': self.seq,\n",
    "            'idx0': index_in_seq0,\n",
    "            'idx1': index_in_seq1,\n",
    "            'pose1': pose0,\n",
    "            'pose2': pose1,\n",
    "            # 'T_cam0_velo': T_cam0_velo,\n",
    "            'T_gt': T_gt,\n",
    "            'cloud0': pc1[:,:3].unsqueeze(0),\n",
    "            'cloud1': pc2[:,:3].unsqueeze(0),\n",
    "            # 'all_matches': list(all_matches),\n",
    "            # 'file_name': file_name\n",
    "            'rep': rep\n",
    "        }\n",
    "    \n",
    "    def process_global_keypoints(self):\n",
    "        pass\n",
    "    \n",
    "    def matching_test_1to2(self, idx, range_of_global_graph = 50, range_of_local_graph = 10):\n",
    "        index_in_seq0 = self.gt_pairs[idx]['anc_idx']\n",
    "        index_in_seq1 = self.gt_pairs[idx]['pos_idx']\n",
    "        while index_in_seq0 <= range_of_global_graph:\n",
    "            idx+=1\n",
    "            index_in_seq0 = self.gt_pairs[idx]['anc_idx']\n",
    "            index_in_seq1 = self.gt_pairs[idx]['pos_idx']\n",
    "            \n",
    "        pose0 = torch.tensor(self.poses[index_in_seq0], dtype=torch.double)\n",
    "        pose1 = torch.tensor(self.poses[index_in_seq1], dtype=torch.double)\n",
    "        T_gt = torch.einsum('ab,de->ae', torch.inverse(pose0), pose1)\n",
    "\n",
    "        pc0_o3d = o3d.geometry.PointCloud()\n",
    "        for i in range(index_in_seq0-range_of_global_graph, index_in_seq0):\n",
    "            pc0_o3d.points.extend(self.dense_scans[i])\n",
    "        pc0_o3d.voxel_down_sample(voxel_size=0.2)\n",
    "        pc0 = np.array(pc0_o3d.points)\n",
    "        pc1_o3d = o3d.geometry.PointCloud()\n",
    "        for i in range(index_in_seq1, index_in_seq1+range_of_local_graph):\n",
    "            pc1_o3d.points.extend(self.dense_scans[i])\n",
    "        pc1_o3d.voxel_down_sample(voxel_size=0.2)\n",
    "        pc1 = np.array(pc1_o3d.points)\n",
    "\n",
    "        kp0_list = []\n",
    "        kp0_for_desc = []\n",
    "        desc0_list = []\n",
    "        for i in range(index_in_seq0-range_of_global_graph, index_in_seq0):\n",
    "            kp0_list.append(self.keypoints[i])\n",
    "            for kp_idx in range(self.keypoints[i].shape[0]):\n",
    "                kp0_for_desc.append(self.keypoints[i][kp_idx])\n",
    "                desc0_list.append(self.descriptors[i][kp_idx])\n",
    "        o3d_kp0 = o3d.geometry.PointCloud()\n",
    "        for i in range(len(kp0_list)):\n",
    "            o3d_kp0.points.extend(kp0_list[i])\n",
    "\n",
    "        labels = np.array(o3d_kp0.cluster_dbscan(eps=0.3, min_points=4, print_progress=False))\n",
    "        if len(labels) > 0:\n",
    "            max_label = labels.max()  # max_label represents the number of clusters\n",
    "            merged_keypoints_chach = [[] for _ in range(max_label+1)]\n",
    "            \n",
    "            # Group keypoints based on their labels (clusters)\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label >= 0:\n",
    "                    merged_keypoints_chach[label].append(np.array(o3d_kp0.points[idx]))\n",
    "            keypoint_chach = []\n",
    "            # Calculate the mean point for each cluster and add it to keypoint_chach\n",
    "            for pi in merged_keypoints_chach:\n",
    "                p = np.mean(pi, axis=0)\n",
    "                keypoint_chach.append(p)\n",
    "        kp0 = np.array(keypoint_chach)\n",
    "\n",
    "        kp1_list = []\n",
    "        kp1_for_desc = []\n",
    "        desc1_list = []\n",
    "        for i in range(index_in_seq1, index_in_seq1 + range_of_local_graph):\n",
    "            kp1_list.append(self.keypoints[i])\n",
    "            for kp_idx in range(self.keypoints[i].shape[0]):\n",
    "                kp1_for_desc.append(self.keypoints[i][kp_idx])\n",
    "                desc1_list.append(self.descriptors[i][kp_idx])\n",
    "        o3d_kp1 = o3d.geometry.PointCloud()\n",
    "        for i in range(len(kp1_list)):\n",
    "            o3d_kp1.points.extend(kp1_list[i])\n",
    "\n",
    "        labels = np.array(o3d_kp1.cluster_dbscan(eps=0.3, min_points=4, print_progress=False))\n",
    "        if len(labels) > 0:\n",
    "            max_label = labels.max()  # max_label represents the number of clusters\n",
    "            merged_keypoints_chach = [[] for _ in range(max_label+1)]\n",
    "            \n",
    "            # Group keypoints based on their labels (clusters)\n",
    "            for idx, label in enumerate(labels):\n",
    "                if label >= 0:\n",
    "                    merged_keypoints_chach[label].append(np.array(o3d_kp1.points[idx]))\n",
    "            keypoint_chach = []\n",
    "            # Calculate the mean point for each cluster and add it to keypoint_chach\n",
    "            for pi in merged_keypoints_chach:\n",
    "                p = np.mean(pi, axis=0)\n",
    "                keypoint_chach.append(p)\n",
    "        kp1 = np.array(keypoint_chach)\n",
    "\n",
    "        kp0_np = np.array([(kp[0], kp[1], kp[2], 1) for kp in kp0])\n",
    "        kp1_np = np.array([(kp[0], kp[1], kp[2], 1) for kp in kp1])\n",
    "        kp0_tensor = torch.tensor(kp0_np, dtype=torch.double)\n",
    "        kp1_tensor = torch.tensor(kp1_np, dtype=torch.double)\n",
    "        kp0_num = len(kp0_tensor)\n",
    "        kp1_num = len(kp1_tensor)\n",
    "\n",
    "        desc0 = []\n",
    "        scores0 = []\n",
    "        desc1 = []\n",
    "        scores1 = []\n",
    "        for i in range(kp0.shape[0]):\n",
    "            min_dist = 100000\n",
    "            min_idx = 0\n",
    "            for j in range(len(kp0_for_desc)):\n",
    "                dist = np.linalg.norm(kp0[i] - kp0_for_desc[j])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_idx = j\n",
    "            kp0[i] = kp0_for_desc[min_idx]\n",
    "            desc0.append(desc0_list[min_idx])\n",
    "            scores0.append(1)\n",
    "        desc0 = np.array(desc0)\n",
    "        scores0 = np.array(scores0)\n",
    "\n",
    "        for i in range(kp1.shape[0]):\n",
    "            min_dist = 100000\n",
    "            min_idx = 0\n",
    "            for j in range(len(kp1_for_desc)):\n",
    "                dist = np.linalg.norm(kp1[i] - kp1_for_desc[j])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_idx = j\n",
    "            kp1[i] = kp1_for_desc[min_idx]\n",
    "            desc1.append(desc1_list[min_idx])\n",
    "            scores1.append(1)\n",
    "        desc1 = np.array(desc1)\n",
    "        scores1 = np.array(scores1)\n",
    "\n",
    "        pc0 = np.array([(kp[0], kp[1], kp[2], 1) for kp in pc0])\n",
    "        pc1 = np.array([(kp[0], kp[1], kp[2], 1) for kp in pc1])\n",
    "        pc0, pc1 = torch.tensor(pc0, dtype=torch.double), torch.tensor(pc1, dtype=torch.double)\n",
    "        pc0 = torch.einsum('ij,nj->ni', torch.inverse(pose0), pc0)\n",
    "        pc1 = torch.einsum('ij,nj->ni', torch.inverse(pose1), pc1)\n",
    "\n",
    "        kp0_local_tensor = torch.einsum('ij,nj->ni', torch.inverse(pose0), kp0_tensor).double()\n",
    "        kp1_local_tensor = torch.einsum('ij,nj->ni', torch.inverse(pose1), kp1_tensor).double()\n",
    "\n",
    "        norm0, norm1 = np.linalg.norm(desc0, axis=1), np.linalg.norm(desc1, axis=1)\n",
    "        norm0, norm1 = norm0.reshape(kp0_num, 1), norm1.reshape(kp1_num, 1)\n",
    "        epsilon = 1e-8  # small constant to prevent division by zero\n",
    "        norm0, norm1 = norm0 + epsilon, norm1 + epsilon\n",
    "        desc0, desc1 = np.where(norm0 != 0, np.multiply(desc0, 1/norm0), 0), np.where(norm1 != 0, np.multiply(desc1, 1/norm1), 0)\n",
    "\n",
    "        desc0_tensor, desc1_tensor = torch.tensor(desc0, dtype=torch.double), torch.tensor(desc1, dtype=torch.double)\n",
    "        scores0_tensor, scores1_tensor = torch.tensor(scores0, dtype=torch.double), torch.tensor(scores1, dtype=torch.double)\n",
    "\n",
    "        dists = cdist(kp0_tensor, kp1_tensor)\n",
    "        '''Find ground true keypoint matching'''\n",
    "        min1 = np.argmin(dists, axis=0)\n",
    "        min2 = np.argmin(dists, axis=1)\n",
    "        min1v = np.min(dists, axis=1)\n",
    "        min1f = min2[min1v < 0.5]\n",
    "\n",
    "        '''For calculating repeatibility'''\n",
    "        rep = len(min1f)\n",
    "\n",
    "        match1, match2 = -1 * np.ones((len(kp0_tensor)), dtype=np.int16), -1 * np.ones((len(kp1_tensor)), dtype=np.int16)\n",
    "        match1[min1v < 0.5] = min1f\n",
    "        min2v = np.min(dists, axis=0)\n",
    "        min2f = min1[min2v < 0.5]\n",
    "        match2[min2v < 0.5] = min2f\n",
    "\n",
    "        return{\n",
    "            # 'skip': False,\n",
    "            'keypoints0': kp0_local_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints1': kp1_local_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints_global_0': kp0_tensor[:,:3].unsqueeze(0),\n",
    "            'keypoints_global_1': kp1_tensor[:,:3].unsqueeze(0),\n",
    "            'descriptors0': desc0_tensor.unsqueeze(0),\n",
    "            'descriptors1': desc1_tensor.unsqueeze(0),\n",
    "            'scores0': scores0_tensor.unsqueeze(0),\n",
    "            'scores1': scores1_tensor.unsqueeze(0),\n",
    "            'gt_matches0': match1,\n",
    "            'gt_matches1': match2,\n",
    "            'sequence': self.seq,\n",
    "            'idx0': index_in_seq0,\n",
    "            'idx1': index_in_seq1,\n",
    "            'pose1': pose0,\n",
    "            'pose2': pose1,\n",
    "            # 'T_cam0_velo': T_cam0_velo,\n",
    "            'T_gt': T_gt,\n",
    "            'cloud0': pc0[:,:3].unsqueeze(0),\n",
    "            'cloud1': pc1[:,:3].unsqueeze(0),\n",
    "            # 'all_matches': list(all_matches),\n",
    "            # 'file_name': file_name\n",
    "            'rep': rep\n",
    "        }\n",
    "    \n",
    "    def global_matching(self, idx):\n",
    "        pass\n",
    "                                     \n",
    "    def __len__(self):\n",
    "        return len(self.poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emplimentation 테스트 시나리오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] 1396's poses SLAM data loaded\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드\n",
    "dataset = Kitti_Rops_dataset_loader(opt, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MDGAT(\n",
       "    (penc): PointnetEncoder(\n",
       "      (sa1): PointNetSetKptsMsg(\n",
       "        (conv_blocks): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (bn_blocks): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (sa2): PointNetSetAbstraction(\n",
       "        (mlp_convs): ModuleList(\n",
       "          (0): Conv2d(131, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (mlp_bns): ModuleList(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (kenc): KeypointEncoder(\n",
       "        (encoder): Sequential(\n",
       "          (0): Conv1d(3, 32, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "          (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): ReLU()\n",
       "          (9): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (gnn): AttentionalGNN(\n",
       "      (layers): ModuleList(\n",
       "        (0): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (3): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (4): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (5): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (6): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (7): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (8): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (9): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (10): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (11): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (12): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (13): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (14): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (15): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (16): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (17): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_proj): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 모델 로드\n",
    "from models.mdgat_denseKITTI import MDGAT\n",
    "path_checkpoint = opt.resume_model\n",
    "checkpoint = torch.load(path_checkpoint, map_location={'cuda:2':'cuda:0'})\n",
    "lr = checkpoint['lr_schedule']\n",
    "config = {\n",
    "        'net': {\n",
    "            'sinkhorn_iterations': opt.sinkhorn_iterations,\n",
    "            'match_threshold': opt.match_threshold,\n",
    "            'lr': lr,\n",
    "            'loss_method': opt.loss_method,\n",
    "            'k': opt.k,\n",
    "            'descriptor': opt.descriptor,\n",
    "            'mutual_check': opt.mutual_check,\n",
    "            'triplet_loss_gamma': opt.triplet_loss_gamma,\n",
    "            'train_step':opt.train_step,\n",
    "            'L':opt.l,\n",
    "            'scheduler_gamma': 0.1**(1/100),\n",
    "            'descriptor_dim': 128,\n",
    "            'keypoint_encoder': [32, 64, 128],\n",
    "            'descritor_encoder': [64, 128],\n",
    "            'GNN_layers': ['self', 'cross'] * 9,\n",
    "        }\n",
    "    }\n",
    "net = MDGAT(config.get('net', {}))\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=config.get('net', {}).get('lr'))\n",
    "net = torch.nn.DataParallel(net)\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.double().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # torch.cuda.set_device(opt.local_rank)\n",
    "    device=torch.device('cuda:{}'.format(opt.local_rank))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"### CUDA not available ###\")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매트릭 코드 짜보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3972/3972 [12:24<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.727, accuracy 0.774, recall 0.752, true match 94.036, false match 19.574, fp_rate 0.254, tp_rate 0.837\n",
      "baned_data 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_test_loss = []; precision_array = []; accuracy_array = []; recall_array = []\n",
    "trans_error_array = []; rot_error_array = []; relative_trans_error_array = []; relative_rot_error_array = []\n",
    "repeatibilty_array = []; valid_num_array = []; all_num_array = []; inlier_array = [] \n",
    "kpnum_array = []; fp_rate_array = []; tp_rate_array = []; tp_rate2_array = []; inlier_ratio_array= [];tm_a=[];fm_a=[]\n",
    "fail = 0\n",
    "baned_data = 0\n",
    "pair_set = tqdm(range(len(dataset.gt_pairs))) \n",
    "for pair in pair_set:\n",
    "# for pair in [0]:\n",
    "    pred = dataset.get_gt_pairs(pair)\n",
    "    for p in pred:\n",
    "        if type(pred[p]) == torch.Tensor:\n",
    "            pred[p] = pred[p].to(device)\n",
    "    # pred['descriptors1'] = net.module.infer_desc(pred['keypoints1'], pred['cloud1'])\n",
    "    # print(pred['descriptors1'].shape)\n",
    "    # pred['descriptors0'] = net.module.infer_desc(pred['keypoints0'], pred['cloud0'])\n",
    "    # print(pred['descriptors0'].shape)\n",
    "    \n",
    "    # print(pred['cloud0'].shape, pred['cloud1'].shape)\n",
    "    data = net.module.infer(pred)\n",
    "    # data = net(pred)\n",
    "    pred = {**pred, **data}\n",
    "\n",
    "    kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "    kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "    matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "    gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "    valid = matches0 > -1\n",
    "    mkpts0 = kpts0[valid]\n",
    "    mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "    mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "    mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "    mutual1 = matches0[mutual0]\n",
    "    x = np.ones(len(matches1)) == 1\n",
    "    x[mutual1] = False\n",
    "    valid1 = matches1 > -1\n",
    "\n",
    "    mconf = conf[valid]\n",
    "\n",
    "    ## ground truth ##\n",
    "    matches_gt, matches_gt1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "    matches_gt[matches_gt == len(matches_gt1)] = -1\n",
    "    matches_gt1[matches_gt1 == len(matches_gt)] = -1\n",
    "    valid_gt = matches_gt > -1\n",
    "\n",
    "    valid_num = np.sum(valid_gt)\n",
    "    all_num = len(valid_gt)\n",
    "    repeatibilty = valid_num/all_num\n",
    "    repeatibilty_array.append(repeatibilty)\n",
    "\n",
    "    mkpts0_gt = kpts0[valid_gt]\n",
    "    mkpts1_gt = kpts1[matches_gt[valid_gt]]\n",
    "    mutual0 = np.arange(len(matches_gt))[valid_gt] == matches_gt1[matches_gt[valid_gt]]\n",
    "    # mutual0_inv = 1-mutual0\n",
    "    mutual0 = np.arange(len(matches_gt))[valid_gt][mutual0]\n",
    "    mutual1 = matches_gt[mutual0]\n",
    "    x = np.ones(len(matches_gt1)) == 1\n",
    "    x[mutual1] = False               \n",
    "    valid_gt1 = matches_gt1 > -1\n",
    "\n",
    "    mscores_gt = pred['scores0'][0].cpu().numpy()[valid_gt]\n",
    "    gt_idx = np.arange(len(kpts0))[valid_gt]\n",
    "\n",
    "    if len(mkpts0) < 4:\n",
    "        fail+=1\n",
    "        print('registration fail')\n",
    "\n",
    "    ''' calculate false positive ,true positive ,true nagetive, precision, accuracy, recall '''\n",
    "    true_positive = [(matches0[i] == matches_gt[i]) and (valid[i]) for i in range(len(kpts0))]\n",
    "    true_negativate = [(matches0[i] == matches_gt[i]) and not (valid[i]) for i in range(len(kpts0))]\n",
    "    false_positive = [valid[i] and (matches_gt[i]==-1) for i in range(len(kpts0))]\n",
    "    ckpts0 = kpts0[true_positive]\n",
    "    ckpts1 = [matches0[true_positive]]\n",
    "    precision = np.sum(true_positive) / np.sum(valid) if np.sum(valid) > 0 else 0\n",
    "    recall = np.sum(true_positive) / np.sum(valid_gt) if np.sum(valid) > 0 else 0\n",
    "    tm = np.sum(true_positive) \n",
    "    fm = np.sum(false_positive) \n",
    "    matching_score = np.sum(true_positive) / len(kpts0) if len(kpts0) > 0 else 0\n",
    "    accuracy = (np.sum(true_positive) + np.sum(true_negativate))/len(matches_gt)\n",
    "    fp_rate = np.sum(false_positive)/np.sum(matches_gt==-1)\n",
    "    tp_rate = np.sum([valid[i] and (matches_gt[i]>-1) for i in range(len(kpts0))])/np.sum(matches_gt > -1)\n",
    "    tp_rate2 = np.sum(true_positive)/np.sum(matches_gt > -1)\n",
    "    T=[]\n",
    "    # print('idx{}, precision {:.3f}, accuracy {:.3f}, recall {:.3f}, true match {:.3f}, false match {:.3f}, fp_rate {:.3f}, tp_rate {:.3f}'.format(\n",
    "    #     pair, precision, accuracy, recall,tm,fm, fp_rate, tp_rate))\n",
    "    precision_array.append(precision)\n",
    "    accuracy_array.append(accuracy)\n",
    "    recall_array.append(recall)\n",
    "    fp_rate_array.append(fp_rate)\n",
    "    tp_rate_array.append(tp_rate)\n",
    "    tp_rate2_array.append(tp_rate2)\n",
    "    tm_a.append(tm)\n",
    "    fm_a.append(fm)\n",
    "precision_mean = np.mean(precision_array)\n",
    "accuracy_mean = np.mean(accuracy_array)\n",
    "recall_mean = np.mean(recall_array)\n",
    "repeatibilty_array_mean = np.mean(repeatibilty_array)\n",
    "fp_rate_mean = np.mean(fp_rate_array)\n",
    "tp_rate_mean = np.mean(tp_rate_array)\n",
    "tp_rate_mean2 = np.mean(tp_rate2_array)\n",
    "tm = np.mean(tm_a)\n",
    "fm = np.mean(fm_a)\n",
    "\n",
    "print('precision {:.3f}, accuracy {:.3f}, recall {:.3f}, true match {:.3f}, false match {:.3f}, fp_rate {:.3f}, tp_rate {:.3f}'.format(\n",
    "        precision_mean, accuracy_mean, recall_mean,tm,fm, fp_rate_mean, tp_rate_mean))\n",
    "# print('average repeatibility: {:.3f}, fail {:.6f}, precision_mean {:.3f}, accuracy_mean {:.3f}, recall_mean {:.3f}, true match {:.3f}, false match {:.3f}, fp_rate_mean {:.3f}, tp_rate_mean {:.3f}, tp_rate_mean2 {:.3f}, trans_error_mean {:.3f}, rot_error_mean {:.3f} '.format(\n",
    "#     repeatibilty_array_mean, fail/pair, precision_mean, accuracy_mean, recall_mean,tm,fm, fp_rate_mean, tp_rate_mean, tp_rate_mean2, trans_error_mean, rot_error_mean ))\n",
    "# print('valid num {}, all_num {}'.format(valid_num_mean, all_num_mean))\n",
    "print('baned_data {}'.format(baned_data))\n",
    "\n",
    "#average repeatibility: 0.593, fail 0.002518, precision_mean 0.947, accuracy_mean 0.944, recall_mean 0.915, true match 111.825, false match 2.146, fp_rate_mean 0.027, tp_rate_mean 0.939, tp_rate_mean2 0.915, trans_error_mean nan, rot_error_mean nan \n",
    "# baned_data 0.0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매칭 비주얼로 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m line_set\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(colors)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpcd_kp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd_kp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_set\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for pair in range(0, len(dataset.gt_pairs), 10):\n",
    "    pred = dataset.get_gt_pairs(pair)\n",
    "    for p in pred:\n",
    "        if type(pred[p]) == torch.Tensor:\n",
    "            pred[p] = pred[p].to(device)\n",
    "    data = net.module.infer(pred)\n",
    "    pred = {**pred, **data}\n",
    "\n",
    "    kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "    kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "    matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "    gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "    valid = matches0 > -1\n",
    "    mkpts0 = kpts0[valid]\n",
    "    mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "    mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "    mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "    mutual1 = matches0[mutual0]\n",
    "    x = np.ones(len(matches1)) == 1\n",
    "    x[mutual1] = False\n",
    "    valid1 = matches1 > -1\n",
    "\n",
    "    mconf = conf[valid]    \n",
    "\n",
    "    pcd_kp0 = o3d.geometry.PointCloud()\n",
    "    pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "    pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "    pcd_kp1 = o3d.geometry.PointCloud()\n",
    "    pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "    pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "    points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "    lines = []\n",
    "    colors = []\n",
    "    for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "        lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "        # lines.append([match, mutual1[idx] + len(kpts_g_0)])\n",
    "        point1 = kpts_g_0[match]\n",
    "        point2 = kpts_g_1[mutual1[idx]]\n",
    "        if np.linalg.norm(point1 - point2) < 1.0:\n",
    "            colors.append([0, 1, 0])\n",
    "        else: \n",
    "            colors.append([1, 0, 0])\n",
    "    line_set = o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector((points)),\n",
    "        lines=o3d.utility.Vector2iVector(lines),\n",
    "    )\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    # o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "    # o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n",
    "    o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제상황처럼 테스트 해보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 184, 3]) torch.Size([1, 179, 3]) torch.Size([1, 184, 135]) torch.Size([1, 179, 135]) torch.Size([1, 184]) torch.Size([1, 179])\n",
      "torch.Size([1, 184, 3]) torch.Size([1, 298, 3]) torch.Size([1, 184, 135]) torch.Size([1, 298, 135]) torch.Size([1, 184]) torch.Size([1, 298])\n",
      "torch.Size([1, 184, 3]) torch.Size([1, 415, 3]) torch.Size([1, 184, 135]) torch.Size([1, 415, 135]) torch.Size([1, 184]) torch.Size([1, 415])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.58 GiB (GPU 0; 23.64 GiB total capacity; 20.27 GiB already allocated; 1.38 GiB free; 20.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         pred[p] \u001b[38;5;241m=\u001b[39m pred[p]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 12\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pred \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata}\n\u001b[1;32m     15\u001b[0m kpts0, kpts1 \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/ADD_prj/MDGAT-matcher/models/mdgat_denseKITTI.py:387\u001b[0m, in \u001b[0;36mMDGAT.infer\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    385\u001b[0m pc0, pc1 \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloud0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdouble(), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloud1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[1;32m    386\u001b[0m desc0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenc(pc0, kpts0)\n\u001b[0;32m--> 387\u001b[0m desc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkpts1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Multi-layer Transformer network.'''\u001b[39;00m\n\u001b[1;32m    390\u001b[0m desc0, desc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn(desc0, desc1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/mdgat/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ADD_prj/MDGAT-matcher/models/mdgat_denseKITTI.py:82\u001b[0m, in \u001b[0;36mPointnetEncoder.forward\u001b[0;34m(self, xyz, kpts)\u001b[0m\n\u001b[1;32m     78\u001b[0m     norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# begin = time.time()\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print('xyz',xyz.shape, 'kp', kpts.shape)\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m l1_xyz, l1_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkpts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# print('sa1',time.time() - begin)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# begin = time.time()\u001b[39;00m\n\u001b[1;32m     85\u001b[0m l2_xyz, l2_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa2(l1_xyz, l1_points)\n",
      "File \u001b[0;32m~/anaconda3/envs/mdgat/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ADD_prj/MDGAT-matcher/models/pointnet/pointnet_util.py:325\u001b[0m, in \u001b[0;36mPointNetSetKptsMsg.forward\u001b[0;34m(self, xyz, points, kpts)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, radius \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mradius_list):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# begin = time()\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsample_list[i]\n\u001b[0;32m--> 325\u001b[0m     group_idx \u001b[38;5;241m=\u001b[39m \u001b[43mquery_ball_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_xyz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# print('query',time()-begin)\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     grouped_xyz \u001b[38;5;241m=\u001b[39m index_points(xyz, group_idx, new_xyz)\n",
      "File \u001b[0;32m~/ADD_prj/MDGAT-matcher/models/pointnet/pointnet_util.py:117\u001b[0m, in \u001b[0;36mquery_ball_point\u001b[0;34m(radius, nsample, xyz, new_xyz)\u001b[0m\n\u001b[1;32m    115\u001b[0m sqrdists \u001b[38;5;241m=\u001b[39m square_distance(new_xyz, xyz)\n\u001b[1;32m    116\u001b[0m group_idx[sqrdists \u001b[38;5;241m>\u001b[39m radius \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m N\n\u001b[0;32m--> 117\u001b[0m group_idx \u001b[38;5;241m=\u001b[39m \u001b[43mgroup_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, :, :nsample]\n\u001b[1;32m    118\u001b[0m group_first \u001b[38;5;241m=\u001b[39m group_idx[:, :, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(B, S, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, nsample])\n\u001b[1;32m    119\u001b[0m mask \u001b[38;5;241m=\u001b[39m group_idx \u001b[38;5;241m==\u001b[39m N\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.58 GiB (GPU 0; 23.64 GiB total capacity; 20.27 GiB already allocated; 1.38 GiB free; 20.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# for pair in range(160, len(dataset.gt_pairs), 10):\n",
    "pair = 140\n",
    "\n",
    "for range0 in [10, 20, 30, 40, 50, 60]:\n",
    "    for range1 in [10, 20, 30, 40, 50, 60]:\n",
    "        pred = dataset.matching_test_1to2(pair, range0, range1)\n",
    "        for p in pred:\n",
    "            if type(pred[p]) == torch.Tensor:\n",
    "                pred[p] = pred[p].to(device)\n",
    "        print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape, pred['scores0'].shape, pred['scores1'].shape)\n",
    "\n",
    "        data = net.module.infer(pred)\n",
    "        pred = {**pred, **data}\n",
    "\n",
    "        kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "        kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "        matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "        gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "        valid = matches0 > -1\n",
    "        mkpts0 = kpts0[valid]\n",
    "        mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "        mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "        mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "        mutual1 = matches0[mutual0]\n",
    "        x = np.ones(len(matches1)) == 1\n",
    "        x[mutual1] = False\n",
    "        valid1 = matches1 > -1\n",
    "\n",
    "        mconf = conf[valid]    \n",
    "\n",
    "        pcd_kp0 = o3d.geometry.PointCloud()\n",
    "        pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "        pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "        pcd_kp1 = o3d.geometry.PointCloud()\n",
    "        pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "        pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "        points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "        lines = []\n",
    "        colors = []\n",
    "        for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "            lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "            # lines.append([match, mutual1[idx] + len(kpts_g_0)])\n",
    "            point1 = kpts_g_0[match]\n",
    "            point2 = kpts_g_1[mutual1[idx]]\n",
    "            if np.linalg.norm(point1 - point2) < 1.0:\n",
    "                colors.append([0, 1, 0])\n",
    "            else: \n",
    "                colors.append([1, 0, 0])\n",
    "        line_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector((points)),\n",
    "            lines=o3d.utility.Vector2iVector(lines),\n",
    "        )\n",
    "        line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "        # o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "        # o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n",
    "        o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in range(160, len(dataset.gt_pairs), 10):\n",
    "pair = 140\n",
    "for range0 in [10, 20, 30, 40, 50, 60]:\n",
    "    for range1 in [10, 20, 30, 40, 50, 60]:\n",
    "        pred = dataset.matching_test_1to2(pair, range0, range1)\n",
    "        for p in pred:\n",
    "            if type(pred[p]) == torch.Tensor:\n",
    "                pred[p] = pred[p].to(device)\n",
    "        print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape, pred['scores0'].shape, pred['scores1'].shape)\n",
    "\n",
    "        data = net.module.infer_mdgat(pred)\n",
    "        pred = {**pred, **data}\n",
    "\n",
    "        kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "        kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "        matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "        gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "        valid = matches0 > -1\n",
    "        mkpts0 = kpts0[valid]\n",
    "        mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "        mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "        mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "        mutual1 = matches0[mutual0]\n",
    "        x = np.ones(len(matches1)) == 1\n",
    "        x[mutual1] = False\n",
    "        valid1 = matches1 > -1\n",
    "\n",
    "        mconf = conf[valid]\n",
    "\n",
    "        gt_match_num = 0\n",
    "        for i in gt_match0:\n",
    "            if i > -1:\n",
    "                gt_match_num+=1\n",
    "                \n",
    "        matched_num = 0\n",
    "        miss_matched_num = 0\n",
    "        for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "            point1 = kpts_g_0[match]\n",
    "            point2 = kpts_g_1[mutual1[idx]]\n",
    "            if np.linalg.norm(point1 - point2) < 0.5:\n",
    "                matched_num+=1\n",
    "            else: \n",
    "                miss_matched_num+=1\n",
    "\n",
    "        print('range0 {}, range1 {}, gt_match_num {}, matched_num {}, miss_matched_num {}'.format(range0, range1, gt_match_num, matched_num, miss_matched_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "kpts_g_0, kpts_g_1 = pred['keypoints_global_0'][0].cpu().numpy(), pred['keypoints_global_1'][0].cpu().numpy()\n",
    "matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "gt_match0, gt_match1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "valid = matches0 > -1\n",
    "mkpts0 = kpts0[valid]\n",
    "mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "mutual1 = matches0[mutual0]\n",
    "x = np.ones(len(matches1)) == 1\n",
    "x[mutual1] = False\n",
    "valid1 = matches1 > -1\n",
    "\n",
    "mconf = conf[valid]\n",
    "\n",
    "# matches_gt, matches_gt1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "# matches_gt[matches_gt == len(matches_gt1)] = -1\n",
    "# matches_gt1[matches_gt1 == len(matches_gt)] = -1\n",
    "# valid_gt = matches_gt > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mutual0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_kp_g_0 = o3d.geometry.PointCloud()\n",
    "pcd_kp_g_0.points = o3d.utility.Vector3dVector(kpts_g_0)\n",
    "pcd_kp_g_0.paint_uniform_color([0, 0, 1])\n",
    "pcd_kp_g_1 = o3d.geometry.PointCloud()\n",
    "pcd_kp_g_1.points = o3d.utility.Vector3dVector(kpts_g_1)\n",
    "pcd_kp_g_1.paint_uniform_color([0, 1, 0])\n",
    "o3d.visualization.draw_geometries([pcd_kp_g_0, pcd_kp_g_1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "pcd_kp_g_0 = o3d.geometry.PointCloud()\n",
    "pcd_kp_g_0.points = o3d.utility.Vector3dVector(kpts_g_0)\n",
    "pcd_kp_g_0.paint_uniform_color([0, 0, 1])\n",
    "pcd_kp_g_1 = o3d.geometry.PointCloud()\n",
    "pcd_kp_g_1.points = o3d.utility.Vector3dVector(kpts_g_1)\n",
    "pcd_kp_g_1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "points = np.concatenate((np.array(pcd_kp_g_0.points),np.array(pcd_kp_g_1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "lines = []\n",
    "colors = []\n",
    "for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "    lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "    # lines.append([match, mutual1[idx] + len(kpts_g_0)])\n",
    "    point1 = kpts_g_0[match]\n",
    "    point2 = kpts_g_1[mutual1[idx]]\n",
    "    if np.linalg.norm(point1 - point2) < 1.0:\n",
    "        colors.append([0, 1, 0])\n",
    "    else: \n",
    "        colors.append([1, 0, 0])\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector((points)),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "# o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n",
    "o3d.visualization.draw_geometries([pcd_kp_g_0, pcd_kp_g_1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kpts0.shape, kpts_g_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dataset.get_divided_data(2)\n",
    "print(pred['keypoints0'].shape, pred['cloud0'].shape, pred['scores0'].shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매칭\n",
    "# pred = dataset.get_matching_data(12)\n",
    "# pred = dataset.get_matching_data(13)\n",
    "# pred = dataset.get_matching_data(12)\n",
    "for p in pred:\n",
    "    if type(pred[p]) == torch.Tensor:\n",
    "        pred[p] = pred[p].to(device)\n",
    "print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape)\n",
    "\n",
    "data = net.module.infer_mdgat(pred, [pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//4, None, pred['keypoints0'].shape[1]//4, None], [pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//4, None, pred['keypoints1'].shape[1]//4, None])\n",
    "pred = {**pred, **data}\n",
    "print(\"pred's keys: \", pred.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 디스크립터 추출\n",
    "net.double().eval()\n",
    "\n",
    "for i in range(dataset.divided_seq_num - 1):\n",
    "    pred = dataset.get_divided_data(i)\n",
    "    for p in pred:\n",
    "        pred[p] = pred[p].to(device)\n",
    "    data = net.module.infer_desc(pred)\n",
    "    for d in data:\n",
    "        data[d] = data[d].detach().cpu()\n",
    "    dataset.push_descriptor(i, data['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 매칭\n",
    "# pred = dataset.get_matching_data(12)\n",
    "pred = dataset.get_matching_data(13)\n",
    "# pred = dataset.get_matching_data(12)\n",
    "for p in pred:\n",
    "    if type(pred[p]) == torch.Tensor:\n",
    "        pred[p] = pred[p].to(device)\n",
    "print(pred['keypoints0'].shape, pred['keypoints1'].shape, pred['descriptors0'].shape, pred['descriptors1'].shape)\n",
    "\n",
    "data = net.module.infer_mdgat(pred, [pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//2, None, pred['keypoints0'].shape[1]//4, None, pred['keypoints0'].shape[1]//4, None], [pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//2, None, pred['keypoints1'].shape[1]//4, None, pred['keypoints1'].shape[1]//4, None])\n",
    "pred = {**pred, **data}\n",
    "print(\"pred's keys: \", pred.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "matches0, matches1, conf = pred['matches0'][0].cpu().detach().numpy(), pred['matches1'][0].cpu().detach().numpy(), pred['matching_scores0'][0].cpu().detach().numpy()\n",
    "valid = matches0 > -1\n",
    "mkpts0 = kpts0[valid]\n",
    "mkpts1 = kpts1[matches0[valid]]\n",
    "\n",
    "mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "mutual1 = matches0[mutual0]\n",
    "x = np.ones(len(matches1)) == 1\n",
    "x[mutual1] = False\n",
    "valid1 = matches1 > -1\n",
    "\n",
    "mconf = conf[valid]\n",
    "\n",
    "matches_gt, matches_gt1 = pred['gt_matches0'], pred['gt_matches1']\n",
    "matches_gt[matches_gt == len(matches_gt1)] = -1\n",
    "matches_gt1[matches_gt1 == len(matches_gt)] = -1\n",
    "valid_gt = matches_gt > -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(matches0.shape, matches1.shape, conf.shape) # >> (3328,) (256,) (3328,)\n",
    "\n",
    "print(matches0[669]) # matches0의 669번째 인덱스와의 매칭 결과 >> 1\n",
    "\n",
    "print(matches1[:]) # matches1의 전체 결과 >> [  -1  669   -1  677 ...], 즉 matches1의 1번째 포인트가 matches0의 669번째 포인트와 매칭됨\n",
    "\n",
    "print(mutual0) # >> [   9   10   13   15 ...]\n",
    "\n",
    "print(mutual1) # >> [248 245  20 255 106 ...]\n",
    "\n",
    "print(matches0[9], matches1[248]) # >> 248, 9, 즉 mutual0[i]와 mutual1[i]의 idx끼리 매칭 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "lines = []\n",
    "for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "    # lines.append([match, match + 1])\n",
    "    # lines.append([mutual1[idx] + len(kpts0), mutual1[idx] + len(kpts0)])\n",
    "    lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "colors = [[0, 1, 0] for _ in range(len(lines))] # lines are shown in green\n",
    "# print(points[lines[0][0]], points[lines[0][1]])\n",
    "# print(pcd_kp0.points[lines[0][0]], pcd_kp1.points[lines[0][1] - len(pcd_kp0.points)])\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector((points)),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = 2.0\n",
    "\n",
    "# 시각화\n",
    "kpts0, kpts1 = pred['keypoints0'][0].cpu().numpy(), pred['keypoints1'][0].cpu().numpy()\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "desc0 = pred['descriptors0'][0].cpu().detach().numpy().T\n",
    "desc1 = pred['descriptors1'][0].cpu().detach().numpy().T\n",
    "dists = cdist(desc0, desc1)\n",
    "min0 = np.argmin(dists, axis=0)\n",
    "min1 = np.argmin(dists, axis=1)\n",
    "min0v = np.min(dists, axis=1)\n",
    "min0f = min1[min0v < ttt]\n",
    "match0, match1 = -1 * np.ones((len(desc0)), dtype=np.int16), -1 * np.ones((len(desc1)), dtype=np.int16)\n",
    "match0[min0v < ttt] = min0f\n",
    "min1v = np.min(dists, axis=0)\n",
    "min1f = min0[min1v < ttt]\n",
    "match1[min1v < ttt] = min1f\n",
    "mutual0 = np.arange(len(matches0))[valid] == matches1[matches0[valid]]\n",
    "mutual0 = np.arange(len(matches0))[valid][mutual0]\n",
    "mutual1 = matches0[mutual0]\n",
    "\n",
    "# 시각화\n",
    "pcd_kp0 = o3d.geometry.PointCloud()\n",
    "pcd_kp0.points = o3d.utility.Vector3dVector(kpts0)\n",
    "pcd_kp0.paint_uniform_color([1, 0, 0])\n",
    "pcd_kp1 = o3d.geometry.PointCloud()\n",
    "pcd_kp1.points = o3d.utility.Vector3dVector(kpts1)\n",
    "pcd_kp1.paint_uniform_color([0, 1, 0])\n",
    "\n",
    "points = np.concatenate((np.array(pcd_kp0.points),np.array(pcd_kp1.points)), axis=0) # >> pcd_kp0에 pcd_kp1를 이어 붙힘\n",
    "lines = []\n",
    "for idx, match in enumerate(mutual0): # mutual0의 값\n",
    "    lines.append([match, mutual1[idx] + len(kpts0)])\n",
    "colors = [[0, 1, 0] for _ in range(len(lines))] # lines are shown in green\n",
    "# print(points[lines[0][0]], points[lines[0][1]])\n",
    "# print(pcd_kp0.points[lines[0][0]], pcd_kp1.points[lines[0][1] - len(pcd_kp0.points)])\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector((points)),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "# o3d.visualization.draw_geometries([pcd0,pcd1,line_set])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd_kp0, pcd_kp1, line_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mutual0)\n",
    "print(mutual1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
