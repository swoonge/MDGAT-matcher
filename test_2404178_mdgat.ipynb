{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, sys, signal,rospy, argparse, csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.distance import cdist\n",
    "import copy, pickle\n",
    "import open3d as o3d\n",
    "import open3d.visualization as vis\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from open3d_ros_helper import open3d_ros_helper as orh\n",
    "from geometry_msgs.msg import Pose, PoseArray, Point # PoseArray, Pose\n",
    "from std_msgs.msg import ColorRGBA\n",
    "from sensor_msgs.msg import PointCloud2\n",
    "from visualization_msgs.msg import Marker\n",
    "\n",
    "from models.mdgat import MDGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='Point cloud matching and pose evaluation',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    slam_dir = '/media/vision/Seagate/DataSets/KRGM/kitti/',\n",
    "    data_folder = 'harris_3D',\n",
    "    local_global = False,\n",
    "    seq_num = '00',\n",
    "    visualize = False,\n",
    "    vis_line_width = 0.2,\n",
    "    calculate_pose = True,\n",
    "    learning_rate = 0.0001,\n",
    "    batch_size = 1,\n",
    "    train_path = './KITTI/',\n",
    "    model_out_path = './models/checkpoint',\n",
    "    memory_is_enough = True,\n",
    "    local_rank = 0,\n",
    "    txt_path = './KITTI/preprocess-random-full',\n",
    "    keypoints_path = './KITTI/keypoints/tsf_256_FPFH_16384-512-k1k16-2d-nonoise',\n",
    "    resume_model = './checkpoint/kitti/mdgat-l9-gap_loss-pointnetmsg-04_01_19_32/train_step3/nomutualcheck-mdgat-batch16-gap_loss-pointnetmsg-USIP-04_01_19_32/best_model_epoch_221(val_loss0.31414552026539594).pth',\n",
    "    loss_method = 'triplet_loss',\n",
    "    net = 'mdgat',\n",
    "    mutual_check = False,\n",
    "    k = [128, None, 128, None, 64, None, 64, None],\n",
    "    l = 9,\n",
    "    descriptor = 'pointnetmsg',\n",
    "    keypoints = 'USIP',\n",
    "    ensure_kpts_num = False,\n",
    "    max_keypoints = -1,\n",
    "    match_threshold = 0.2,\n",
    "    threshold = 0.5,\n",
    "    triplet_loss_gamma = 0.5,\n",
    "    sinkhorn_iterations = 20,\n",
    "    train_step = 3,\n",
    "\n",
    "    fpfh_normal_radiuse = 0.3,\n",
    "    fpfh_descriptors_radiuse = 1.0,\n",
    "    seq_list = [0],\n",
    "    mdgat_path = './KITTI',\n",
    "    kitti_path = '/media/vision/Seagate/DataSets/kitti/dataset/sequences',\n",
    "    transform_opt = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_sigint(signal, frame):\n",
    "    print(\"\\n ---cancel by user---\")\n",
    "    sys.exit(0)\n",
    "\n",
    "def model_inference(net, data, device):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        data['keypoints0'] = data['keypoints0'].to(device)\n",
    "        data['keypoints1'] = data['keypoints1'].to(device)\n",
    "        data['cloud0'] = data['cloud0'].to(device)\n",
    "        data['cloud1'] = data['cloud1'].to(device)\n",
    "        data['scores0'] = data['cloud0'].to(device)\n",
    "        data['scores1'] = data['cloud1'].to(device)\n",
    "\n",
    "        output = net(data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset():\n",
    "    def __init__(self, args) -> None:\n",
    "        self.gt_seq = args.seq_num\n",
    "        self.data_folder = args.data_folder\n",
    "\n",
    "        self.dir_SLAM_path = args.slam_dir + self.data_folder +\"/\"\n",
    "        self.descriptor_type = args.descriptor\n",
    "\n",
    "        self.poses = []\n",
    "        self.dense_scans = []\n",
    "        self.keypoints = []\n",
    "        self.descriptors = []\n",
    "        self.local_graph_range = [0, 0]\n",
    "        self.divided_keypoints = np.array([])\n",
    "        self.divided_dense_scans = []\n",
    "\n",
    "        self._get_SLAM_poses()\n",
    "        self._get_dense_frames()\n",
    "        self._get_keypoints()\n",
    "        self._get_descriptors()\n",
    "        print(\"[Load] %d's poses SLAM data loaded\" % len(self.poses))\n",
    "\n",
    "    def _get_SLAM_poses(self):\n",
    "        with open(file=os.path.join(self.dir_SLAM_path, \"Poses_kitti_\" + self.gt_seq + \".pickle\"), mode='rb') as f:\n",
    "            self.poses = pickle.load(f)\n",
    "\n",
    "    def _get_dense_frames(self):\n",
    "        with open(file=os.path.join(self.dir_SLAM_path, \"DenseFrames_kitti_\" + self.gt_seq + \".pickle\"), mode='rb') as f:\n",
    "            self.dense_scans = pickle.load(f)\n",
    "\n",
    "    def _get_keypoints(self):\n",
    "        with open(file=os.path.join(self.dir_SLAM_path, \"keyPoints_kitti_\" + self.gt_seq + \".pickle\"), mode='rb') as f:\n",
    "            self.keypoints = pickle.load(f) # (pose_num, keypoint_num, 3), (keypoint_num, 3)는 np.array\n",
    "\n",
    "    def _get_descriptors(self):\n",
    "        if self.descriptor_type == \"FPFH\":\n",
    "            with open(file=os.path.join(self.dir_SLAM_path, \"Descriptors_FPFH_kitti_\" + self.gt_seq + \".pickle\"), mode='rb') as f:\n",
    "                self.descriptors = pickle.load(f)\n",
    "        elif self.descriptor_type == \"pointnet\" or \"pointnetmsg\":\n",
    "            for kps in self.keypoints:\n",
    "                self.descriptors.append(torch.zeros((kps.shape[0], 128)))\n",
    "\n",
    "    def divide_data(self, divide_num = 256):\n",
    "        # divide keypoints\n",
    "        flatten_keypoints = []\n",
    "        for kps in self.keypoints:\n",
    "            for kp in kps:\n",
    "                flatten_keypoints.append(np.array(kp))\n",
    "        flatten_keypoints = np.array(flatten_keypoints) # (n, 3)\n",
    "        sample_num = flatten_keypoints.shape[0] // (divide_num)\n",
    "        flatten_keypoints = flatten_keypoints[:(divide_num) * sample_num]\n",
    "        self.divided_keypoints = flatten_keypoints.reshape((sample_num, divide_num, 3))\n",
    "\n",
    "        # devide dense_scans\n",
    "        n = 0\n",
    "        divided_pcd = o3d.geometry.PointCloud()\n",
    "        for idx, kps in enumerate(self.keypoints):\n",
    "            divided_pcd.points.extend(self.dense_scans[idx])\n",
    "            for _ in kps:\n",
    "                n += 1\n",
    "                if n % divide_num == 0:\n",
    "                    divided_pcd = divided_pcd.voxel_down_sample(voxel_size=0.2)\n",
    "                    self.divided_dense_scans.append(np.array(divided_pcd.points))\n",
    "                    divided_pcd.clear()\n",
    "                    divided_pcd.points.extend(self.dense_scans[idx])\n",
    "                    n = 0\n",
    "        print(\"[Divide] Keypoints and dense scan divided into %d-num seq\" % (sample_num))\n",
    "\n",
    "    def get_divided_data(self, idx):\n",
    "        if idx >= len(self.divided_keypoints) - 1:\n",
    "            idx = len(self.divided_keypoints) - 1\n",
    "            print(\"[Warning] idx is over the divided keypoints, so return the last one\")\n",
    "        infer_kp = torch.tensor(self.divided_keypoints[idx], dtype=torch.double).unsqueeze(0)\n",
    "        infer_pc = torch.tensor(self.divided_dense_scans[idx], dtype=torch.double)\n",
    "        infer_pc = torch.cat((infer_pc, torch.ones(infer_pc.shape[0], 1)), dim=1)\n",
    "        \n",
    "        n = infer_pc.shape[0] // 2\n",
    "        infer_pc = infer_pc[:n*2,:]\n",
    "        infer_pc = infer_pc.reshape((-1, 8)).unsqueeze(0)\n",
    "        \n",
    "        infer_scores = torch.ones((infer_kp.shape[1])).unsqueeze(0)\n",
    "\n",
    "        infer_kp.to('cuda')\n",
    "        infer_pc.to('cuda')\n",
    "        infer_scores.to('cuda')\n",
    "\n",
    "        return{\n",
    "            'keypoints0': infer_kp,\n",
    "            'cloud0': infer_pc,\n",
    "            'scores0': infer_scores\n",
    "        } \n",
    "    \n",
    "    def push_descriptor(self, descriptors):\n",
    "        for desc in descriptors:\n",
    "            self.descriptors.append(desc)\n",
    "\n",
    "    def get_matching_data(self, idx):\n",
    "        if idx >= len(self.divided_keypoints) - 1:\n",
    "            idx = len(self.divided_keypoints) - 1\n",
    "            print(\"[Warning] idx is over the divided keypoints, so return the last one\")\n",
    "\n",
    "        global_keypoints = np.zeros((1,3))\n",
    "        global_descriptors = np.zeros((1,256))\n",
    "\n",
    "        for i in range(idx - 1):\n",
    "            global_keypoints = np.vstack((global_keypoints, self.divided_keypoints[i]))\n",
    "            global_descriptors = np.vstack((global_descriptors, self.divided_dense_scans[i]))\n",
    "        local_keypoints = self.divided_keypoints[idx]\n",
    "        local_descriptors = self.divided_dense_scans[idx]\n",
    "                                     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset(opt)\n",
    "dataset.divide_data(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(dataset.divided_dense_scans)+5):\n",
    "    kp, pc = dataset.get_divided_data(idx)\n",
    "    print(idx,\"-th data:\", kp.shape, pc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dataset' object has no attribute 'divided_keypoins'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mdivided_dense_scans)):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m( idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-th section: \u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mdivided_dense_scans[idx]\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivided_keypoins\u001b[49m[idx]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m     pcd_pc \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mPointCloud()\n\u001b[1;32m      4\u001b[0m     pcd_pc\u001b[38;5;241m.\u001b[39mpoints \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(dataset\u001b[38;5;241m.\u001b[39mdivided_dense_scans[idx])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dataset' object has no attribute 'divided_keypoins'"
     ]
    }
   ],
   "source": [
    "for idx in range(len(dataset.divided_dense_scans)):\n",
    "    print( idx, \"-th section: \", dataset.divided_dense_scans[idx].shape, \", \", dataset.divided_keypoins[idx].shape)\n",
    "    pcd_pc = o3d.geometry.PointCloud()\n",
    "    pcd_pc.points = o3d.utility.Vector3dVector(dataset.divided_dense_scans[idx])\n",
    "    pcd_pc.colors = o3d.utility.Vector3dVector(np.ones((dataset.divided_dense_scans[idx].shape[0], 3)) * [0.5, 0.5, 0.5])\n",
    "    pcd_kp = o3d.geometry.PointCloud()\n",
    "    pcd_kp.points = o3d.utility.Vector3dVector(dataset.divided_keypoins[idx])\n",
    "    pcd_kp.colors = o3d.utility.Vector3dVector(np.ones((dataset.divided_keypoins[idx].shape[0], 3)) * [1, 0, 0])\n",
    "\n",
    "    # Visualize the point clouds\n",
    "    # vis.draw_geometries([pcd_pc, pcd_kp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(dataset.divided_keypoints)):\n",
    "    kp, desc = dataset.get_matching_data(idx)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emplimentation 테스트 시나리오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] 929's poses SLAM data loaded\n",
      "[Divide] Keypoints and dense scan divided into 42-num seq\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 및 분할\n",
    "dataset = dataset(opt)\n",
    "dataset.divide_data(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 3]) torch.Size([1, 54621, 8]) torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "pred = dataset.get_divided_data(2)\n",
    "print(pred['keypoints0'].shape, pred['cloud0'].shape, pred['scores0'].shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MDGAT(\n",
       "    (penc): PointnetEncoderMsg(\n",
       "      (sa1): PointNetSetKptsMsg(\n",
       "        (conv_blocks): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (bn_blocks): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (sa2): PointNetSetAbstraction(\n",
       "        (mlp_convs): ModuleList(\n",
       "          (0): Conv2d(323, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (mlp_bns): ModuleList(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (kenc): KeypointEncoder(\n",
       "        (encoder): Sequential(\n",
       "          (0): Conv1d(4, 32, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "          (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "          (6): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "          (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (8): ReLU()\n",
       "          (9): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (gnn): AttentionalGNN(\n",
       "      (layers): ModuleList(\n",
       "        (0): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (1): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (3): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (4): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (5): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (6): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (7): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (8): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (9): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (10): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (11): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (12): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (13): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (14): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (15): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (16): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (17): AttentionalPropagation(\n",
       "          (attn): MultiHeadedAttention(\n",
       "            (merge): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (proj): ModuleList(\n",
       "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "              (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_proj): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 모델 로드\n",
    "path_checkpoint = opt.resume_model  \n",
    "checkpoint = torch.load(path_checkpoint, map_location={'cuda:2':'cuda:0'})  \n",
    "lr = checkpoint['lr_schedule']\n",
    "config = {\n",
    "    'net': {\n",
    "        'sinkhorn_iterations': opt.sinkhorn_iterations,\n",
    "        'match_threshold': opt.match_threshold,\n",
    "        'lr': opt.learning_rate,\n",
    "        'loss_method': opt.loss_method,\n",
    "        'k': opt.k,\n",
    "        'descriptor': opt.descriptor,\n",
    "        'mutual_check': opt.mutual_check,\n",
    "        'triplet_loss_gamma': opt.triplet_loss_gamma,\n",
    "        'train_step':opt.train_step,\n",
    "        'L':opt.l\n",
    "    }\n",
    "}\n",
    "net = MDGAT(config.get('net', {}))\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=config.get('net', {}).get('lr'))\n",
    "net = torch.nn.DataParallel(net)\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # torch.cuda.set_device(opt.local_rank)\n",
    "    device=torch.device('cuda:{}'.format(opt.local_rank))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"### CUDA not available ###\")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 디스크립터 추출\n",
    "net.double().eval()\n",
    "pred = dataset.get_divided_data(2)\n",
    "\n",
    "for p in pred:\n",
    "    pred[p] = pred[p].to(device)\n",
    "\n",
    "data = net.module.infer(pred)\n",
    "pred = {**pred, **data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['desc'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
