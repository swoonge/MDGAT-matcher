{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 디스크립터 중 모든 값이 0인 디스크립터의 비율 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import open3d.visualization as vis\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mdgat_kitti_data_set():\n",
    "    def __init__(self, opt):\n",
    "        self.kitti_path = opt.kitti_path\n",
    "        self.mdgat_path = opt.mdgat_path\n",
    "        self.seq_list = opt.seq_list\n",
    "        self.memory_is_enough = opt.memory_is_enough\n",
    "        self.mutual_check = opt.mutual_check\n",
    "        self.transform_opt = opt.transform_opt\n",
    "        self.fpfh_normal_radiuse = opt.fpfh_normal_radiuse\n",
    "        self.fpfh_descriptors_radiuse = opt.fpfh_descriptors_radiuse\n",
    "\n",
    "        self.dataset = []\n",
    "        self.calib = {}\n",
    "        self.pose = {}\n",
    "        self.kp = {}\n",
    "        self.desc = {}\n",
    "        self.scores = {}\n",
    "        self.scan = {}\n",
    "        self.random_sample_num = 16384 #16384\n",
    "        self.threshold = 0.5\n",
    "\n",
    "        self._load_kitti_gt_txt()\n",
    "        self._load_preprocessed_data()\n",
    "\n",
    "        print('Data loaded')\n",
    "        # for seq in self.seq_list:\n",
    "            # print('    Sequence %02d has %d pose/ %d kp'%(seq, len(self.kp[seq]), self.kp[seq][0].shape[0]))\n",
    "\n",
    "    def _load_kitti_gt_txt(self):\n",
    "        '''\n",
    "        :param txt_root:\n",
    "        :param seq\n",
    "        :return: [{anc_idx: *, pos_idx: *, seq: *}]                \n",
    "        '''\n",
    "        for seq in self.seq_list:\n",
    "            with open(os.path.join(self.mdgat_path, 'preprocess-random-full','%02d'%seq, 'groundtruths.txt'), 'r') as f:\n",
    "                lines_list = f.readlines()\n",
    "                for i, line_str in enumerate(lines_list):\n",
    "                    if i == 0:\n",
    "                        # skip the header line\n",
    "                        continue\n",
    "                    line_splitted = line_str.split()\n",
    "                    anc_idx = int(line_splitted[0])\n",
    "                    pos_idx = int(line_splitted[1])\n",
    "\n",
    "                    data = {'seq': seq, 'anc_idx': anc_idx, 'pos_idx': pos_idx}\n",
    "                    self.dataset.append(data)\n",
    "    \n",
    "    def _load_preprocessed_data(self):\n",
    "        for seq in self.seq_list:\n",
    "            sequence = '%02d'%seq\n",
    "            calibpath = os.path.join(self.mdgat_path, 'calib/sequences', sequence, 'calib.txt')\n",
    "            posepath = os.path.join(self.mdgat_path, 'poses', '%02d.txt'%seq)\n",
    "            with open(calibpath, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    _, value = line.split(':', 1)\n",
    "                    try:\n",
    "                        calib = np.array([float(x) for x in value.split()])\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                    calib = np.reshape(calib, (3, 4))    \n",
    "                    self.calib[sequence] = np.vstack([calib, [0, 0, 0, 1]])\n",
    "            \n",
    "            poses = []\n",
    "            with open(posepath, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    T_w_cam0 = np.fromstring(line, dtype=float, sep=' ')\n",
    "                    T_w_cam0 = T_w_cam0.reshape(3, 4)\n",
    "                    T_w_cam0 = np.vstack((T_w_cam0, [0, 0, 0, 1]))\n",
    "                    poses.append(T_w_cam0)\n",
    "                self.pose[sequence] = poses\n",
    "\n",
    "            '''If memory is enough, load all the data'''\n",
    "            if self.memory_is_enough:\n",
    "                kps = []\n",
    "                scores = []\n",
    "                desc = []\n",
    "                folder = os.path.join(self.mdgat_path, 'keypoints/tsf_256_FPFH_16384-512-k1k16-2d-nonoise', sequence)\n",
    "                folder = os.listdir(folder)   \n",
    "                folder.sort(key=lambda x:int(x[:-4]))\n",
    "                for idx in range(len(folder)):\n",
    "                    file = os.path.join(self.mdgat_path, 'keypoints/tsf_256_FPFH_16384-512-k1k16-2d-nonoise', sequence, folder[idx])\n",
    "                    if os.path.isfile(file):\n",
    "                        pc = np.reshape(np.fromfile(file, dtype=np.float32), (-1, 37))\n",
    "                        ones = np.ones((pc.shape[0], 1))\n",
    "                        kps.append(np.concatenate((pc[:,:3], ones), axis=1))\n",
    "                        scores.append(pc[:,3])\n",
    "                        desc.append(pc[:,4:])\n",
    "                    else:\n",
    "                        kps.append([0])\n",
    "\n",
    "                self.kp[sequence] = kps\n",
    "                self.desc[sequence] = desc\n",
    "                self.scores[sequence] = scores\n",
    "\n",
    "    def _get_kitti_data(self, sequence, index_in_seq):\n",
    "        pc_file = os.path.join('/media/vision/Seagate/DataSets/kitti/dataset/sequences', sequence, \"velodyne\" ,'%06d.bin' % index_in_seq)\n",
    "        pc = np.fromfile(pc_file, dtype=np.float32)\n",
    "        pc = pc.reshape((-1, 4))\n",
    "\n",
    "        ones = np.ones((pc.shape[0], 1))\n",
    "        pc = np.concatenate((pc[:,:3], ones), axis=1)\n",
    "        pc = torch.tensor(pc, dtype=torch.double)\n",
    "        return pc\n",
    "    \n",
    "    def _get_kitti_poseset(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        sequence = '%02d'%data['seq']\n",
    "        anc_idx = data['anc_idx']\n",
    "        pos_idx = data['pos_idx']\n",
    "        pc0 = self._get_kitti_data(sequence, anc_idx)\n",
    "        pc1 = self._get_kitti_data(sequence, pos_idx)\n",
    "        return pc0, pc1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def set_fpfh_radiuse(self, radiuse, descriptors_radiuse):\n",
    "        self.fpfh_normal_radiuse = radiuse\n",
    "        self.fpfh_descriptors_radiuse = descriptors_radiuse\n",
    "    \n",
    "    def comute_FPFH(self, pc, kp):\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(pc[:, :3])\n",
    "\n",
    "        kp_num = kp.shape[0]\n",
    "        pcd_kp = o3d.geometry.PointCloud()\n",
    "        pcd_kp.points = o3d.utility.Vector3dVector(kp[:, :3])\n",
    "        pcd += pcd_kp\n",
    "\n",
    "        pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=self.fpfh_normal_radiuse, max_nn=30))\n",
    "\n",
    "        pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(pcd, o3d.geometry.KDTreeSearchParamHybrid(radius=self.fpfh_normal_radiuse, max_nn=100))\n",
    "        fpfh_desc = np.transpose(pcd_fpfh.data)[-kp_num:]\n",
    "        fpfh_desc = torch.tensor(fpfh_desc, dtype=torch.double)\n",
    "        return fpfh_desc\n",
    "    \n",
    "    def test_FPFH(self, seq, idx):\n",
    "        sequence = '%02d'%seq\n",
    "        pc = self._get_kitti_data(sequence, idx)\n",
    "        kp = self.kp[sequence][idx]\n",
    "        fpfh_desc = self.comute_FPFH(pc, kp)\n",
    "        return fpfh_desc\n",
    "    \n",
    "    def get_data(self, seq, idx):\n",
    "        # data = self.dataset[idx]\n",
    "        sequence = '%02d'%seq\n",
    "\n",
    "        pose = torch.tensor(self.pose[sequence][idx], dtype=torch.double)\n",
    "        kp = torch.tensor(self.kp[sequence][idx], dtype=torch.double)\n",
    "        desc = torch.tensor(self.desc[sequence][idx], dtype=torch.double)\n",
    "        score = torch.tensor(self.scores[sequence][idx], dtype=torch.double)\n",
    "        pc = self._get_kitti_data(sequence, idx)\n",
    "        fpfh_desc = self.comute_FPFH(pc, kp)\n",
    "\n",
    "        T_cam0_velo = self.calib[sequence]\n",
    "        T_cam0_velo = torch.tensor(T_cam0_velo, dtype=torch.double)\n",
    "\n",
    "        '''transform point cloud from cam0 to LiDAR'''\n",
    "        kpw = torch.einsum('ki,ij,jm->mk', pose, T_cam0_velo, kp.T)\n",
    "        pcw = torch.einsum('ki,ij,jm->mk', pose, T_cam0_velo, pc.T)\n",
    "\n",
    "        top_k = 10\n",
    "        top_scores, top_indices = torch.topk(score, k=top_k)\n",
    "        kp = kp[top_indices]\n",
    "        kpw = kpw[top_indices]\n",
    "        desc = desc[top_indices]\n",
    "\n",
    "        norm = np.linalg.norm(desc, axis=1)\n",
    "        norm = norm.reshape(len(kp), 1)\n",
    "        desc = np.where(norm != 0, np.multiply(desc, 1/norm), 0)\n",
    "\n",
    "        fpfh_norm = np.linalg.norm(fpfh_desc, axis=1)\n",
    "        fpfh_norm = fpfh_norm.reshape(len(fpfh_desc), 1)\n",
    "        fpfh_desc = np.where(fpfh_norm != 0, np.multiply(fpfh_desc, 1/fpfh_norm), 0)\n",
    "\n",
    "        return {\n",
    "            # 'skip': False,\n",
    "            'keypoints': kp,\n",
    "            'keypointsw': kpw,\n",
    "            'descriptors': desc,\n",
    "            'fpfh_descriptors': fpfh_desc,\n",
    "            'scores': score,\n",
    "            'sequence': sequence,\n",
    "            'idx': idx,\n",
    "            'pose': pose,\n",
    "            'T_cam0_velo': T_cam0_velo,\n",
    "            'cloud': pc,\n",
    "            'cloudw': pcw,\n",
    "        } \n",
    "    \n",
    "    def get_pair_data(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        sequence = '%02d'%data['seq']\n",
    "        anc_idx = data['anc_idx']\n",
    "        pos_idx = data['pos_idx']\n",
    "        pose0 = torch.tensor(self.pose[sequence][anc_idx], dtype=torch.double)\n",
    "        pose1 = torch.tensor(self.pose[sequence][pos_idx], dtype=torch.double)\n",
    "\n",
    "        kp0 = torch.tensor(self.kp[sequence][anc_idx], dtype=torch.double)\n",
    "        kp1 = torch.tensor(self.kp[sequence][pos_idx], dtype=torch.double)\n",
    "\n",
    "        desc0 = torch.tensor(self.desc[sequence][anc_idx], dtype=torch.double)\n",
    "        desc1 = torch.tensor(self.desc[sequence][pos_idx], dtype=torch.double)\n",
    "\n",
    "        score0 = torch.tensor(self.scores[sequence][anc_idx], dtype=torch.double)\n",
    "        score1 = torch.tensor(self.scores[sequence][pos_idx], dtype=torch.double)\n",
    "\n",
    "        pc0, pc1 = self._get_kitti_poseset(idx)\n",
    "        pc0 = pc0.clone().detach()\n",
    "        pc1 = pc1.clone().detach()\n",
    "\n",
    "        fpfh_desc0 = self.comute_FPFH(pc0, kp0)\n",
    "        fpfh_desc1 = self.comute_FPFH(pc1, kp1)\n",
    "\n",
    "        T_cam0_velo = self.calib[sequence]\n",
    "        T_cam0_velo = torch.tensor(T_cam0_velo, dtype=torch.double)\n",
    "        T_gt = torch.einsum('ab,bc,cd,de->ae', torch.inverse(T_cam0_velo), torch.inverse(pose0), pose1, T_cam0_velo) # T_gt: transpose kp2 to kp1\n",
    "\n",
    "        '''transform point cloud from cam0 to LiDAR'''\n",
    "        kp0w_np = torch.einsum('ki,ij,jm->mk', pose0, T_cam0_velo, kp0.T)\n",
    "        kp1w_np = torch.einsum('ki,ij,jm->mk', pose1, T_cam0_velo, kp1.T)\n",
    "        pc0w = torch.einsum('ki,ij,jm->mk', pose0, T_cam0_velo, pc0.T)\n",
    "        pc1w = torch.einsum('ki,ij,jm->mk', pose1, T_cam0_velo, pc1.T)\n",
    "\n",
    "        fpfh_desc0 = torch.tensor(self.comute_FPFH(pc0, kp0), dtype=torch.double)\n",
    "        fpfh_desc0 = torch.tensor(self.comute_FPFH(pc1, kp1), dtype=torch.double)\n",
    "\n",
    "        '''transform pose from cam0 to LiDAR'''\n",
    "        kp0w_np = kp0w_np[:, :3]\n",
    "        kp1w_np = kp1w_np[:, :3]\n",
    "        dists = cdist(kp0w_np, kp1w_np)\n",
    "        '''Find ground true keypoint matching'''\n",
    "        min0 = np.argmin(dists, axis=0)\n",
    "        min1 = np.argmin(dists, axis=1)\n",
    "        min0v = np.min(dists, axis=1)\n",
    "        min0f = min1[min0v < self.threshold]\n",
    "        '''For calculating repeatibility'''\n",
    "        rep = len(min0f)\n",
    "        '''\n",
    "        If you got high-quality keypoints, you can set the \n",
    "        mutual_check to True, otherwise, it is better to \n",
    "        set to False\n",
    "        '''\n",
    "        match0, match1 = -1 * np.ones((len(kp0)), dtype=np.int16), -1 * np.ones((len(kp1)), dtype=np.int16)\n",
    "        if self.mutual_check:\n",
    "            xx = np.where(min1[min0] == np.arange(min0.shape[0]))[0]\n",
    "            matches = np.intersect1d(min0f, xx)\n",
    "\n",
    "            match0[min0[matches]] = matches\n",
    "            match1[matches] = min0[matches]\n",
    "        else:\n",
    "            match0[min0v < self.threshold] = min0f\n",
    "\n",
    "            min1v = np.min(dists, axis=0)\n",
    "            min1f = min0[min1v < self.threshold]\n",
    "            match1[min1v < self.threshold] = min1f\n",
    "\n",
    "        kp0 = kp0[:, :3]\n",
    "        kp1 = kp1[:, :3]\n",
    "\n",
    "        norm0, norm1 = np.linalg.norm(desc0, axis=1), np.linalg.norm(desc1, axis=1)\n",
    "        norm0, norm1 = norm0.reshape(len(kp0), 1), norm1.reshape(len(kp1), 1)\n",
    "        desc0, desc1  = np.multiply(desc0, 1/norm0), np.multiply(desc1, 1/norm1)\n",
    "        # desc0, desc1 = np.where(norm0 != 0, np.multiply(desc0, 1/norm0), 0), np.where(norm1 != 0, np.multiply(desc1, 1/norm1), 0)\n",
    "\n",
    "        fpfh_norm0, fpfh_norm1 = np.linalg.norm(fpfh_desc0, axis=1), np.linalg.norm(fpfh_desc1, axis=1)\n",
    "        fpfh_norm0, fpfh_norm1 = fpfh_norm0.reshape(len(kp0), 1), fpfh_norm1.reshape(len(kp1), 1)\n",
    "        fpfh_desc0, fpfh_desc1 = np.where(fpfh_norm0 != 0, np.multiply(fpfh_desc0, 1/fpfh_norm0), 0), np.where(fpfh_norm1 != 0, np.multiply(fpfh_desc1, 1/fpfh_norm1), 0)\n",
    "        # fpfh_desc0, fpfh_desc1  = np.multiply(fpfh_desc0, 1/fpfh_norm0), np.multiply(fpfh_desc1, 1/fpfh_norm1)\n",
    "\n",
    "        fpfh_desc0 = torch.tensor(self.comute_FPFH(pc0, kp0), dtype=torch.double)\n",
    "        fpfh_desc0 = torch.tensor(self.comute_FPFH(pc1, kp1), dtype=torch.double)\n",
    "        \n",
    "        return {\n",
    "            # 'skip': False,\n",
    "            'keypoints0': kp0,\n",
    "            'keypoints1': kp1,\n",
    "            'keypointsw0': kp0w_np,\n",
    "            'keypointsw1': kp1w_np,\n",
    "            'descriptors0': desc0,\n",
    "            'descriptors1': desc1,\n",
    "            'fpfh_descriptors0': fpfh_desc0,\n",
    "            'fpfh_descriptors1': fpfh_desc1,\n",
    "            'scores0': score0,\n",
    "            'scores1': score1,\n",
    "            'gt_matches0': match0,\n",
    "            'gt_matches1': match1,\n",
    "            'sequence': sequence,\n",
    "            'idx0': anc_idx,\n",
    "            'idx1': pos_idx,\n",
    "            'pose0': pose0,\n",
    "            'pose1': pose1,\n",
    "            'T_cam0_velo': T_cam0_velo,\n",
    "            'T_gt': T_gt,\n",
    "            'cloud0': pc0,\n",
    "            'cloud1': pc1,\n",
    "            'cloudw0': pc0w,\n",
    "            'cloudw1': pc1w,\n",
    "            # 'all_matches': list(all_matches),\n",
    "            # 'file_name': file_name\n",
    "            'rep': rep\n",
    "        } \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디스크립터가 0가 나오는지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fpfh_normal_radiuses = [1.0]\n",
    "test_fpfh_descriptors_radiuses = [1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "1.0 1.5 sequence 00 , entire norm num: 11776 , zeros norm num: 0 , ratio of zeros: 0.0000 , max zeros num: 0 / 256\n"
     ]
    }
   ],
   "source": [
    "# # test_fpfh_normal_radiuses = [0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "# # test_fpfh_descriptors_radiuses = [0.25, 0.5, 0.7, 1.0, 1.2, 1.5]\n",
    "# test_fpfh_normal_radiuses = [0.05]\n",
    "# test_fpfh_descriptors_radiuses = [0.15]\n",
    "opt = argparse.Namespace(\n",
    "    fpfh_normal_radiuse = 0.1,\n",
    "    fpfh_descriptors_radiuse = 0.25,\n",
    "    seq_list = [0],\n",
    "    mdgat_path = './KITTI',\n",
    "    kitti_path = '/media/vision/Seagate/DataSets/kitti/dataset/sequences',\n",
    "    memory_is_enough = True,\n",
    "    mutual_check = False,\n",
    "    transform_opt = 0\n",
    ")\n",
    "data_set = mdgat_kitti_data_set(opt)\n",
    "\n",
    "entire_norm_num_list = []\n",
    "zeros_norm_num_list = []\n",
    "max_zeros_num_list = []\n",
    "\n",
    "for fpfh_normal_radiuse in test_fpfh_normal_radiuses:\n",
    "    for fpfh_descriptors_radiuse in test_fpfh_descriptors_radiuses:\n",
    "        data_set.set_fpfh_radiuse(fpfh_normal_radiuse, fpfh_descriptors_radiuse)\n",
    "        for seq in opt.seq_list:\n",
    "            seq_norm_num = 0\n",
    "            seq_zeros_num = 0\n",
    "            max_zeros_num = 0\n",
    "            sequence = '%02d'%seq\n",
    "\n",
    "            for idx in range(0, len(data_set.kp[sequence]), 100):\n",
    "                data = data_set.get_data(seq, idx)\n",
    "                desc = data['fpfh_descriptors']\n",
    "                # print(desc)\n",
    "                norm = np.linalg.norm(desc, axis=1)\n",
    "                seq_norm_num += len(norm)\n",
    "                seq_zeros_num += len(norm) - np.count_nonzero(norm)\n",
    "                if max_zeros_num < len(norm) - np.count_nonzero(norm):\n",
    "                    max_zeros_num = len(norm) - np.count_nonzero(norm)\n",
    "            entire_norm_num_list.append(seq_norm_num)\n",
    "            zeros_norm_num_list.append(seq_zeros_num)\n",
    "            max_zeros_num_list.append([max_zeros_num, 256])\n",
    "\n",
    "        for seq in opt.seq_list:\n",
    "            # print((entire_norm_num_list[seq] / zeros_norm_num_list[seq]))\n",
    "            print(fpfh_normal_radiuse, fpfh_descriptors_radiuse, \"sequence %02d\" % seq, \n",
    "                \", entire norm num: %d\" % entire_norm_num_list[seq], \n",
    "                \", zeros norm num: %d\" % zeros_norm_num_list[seq], \n",
    "                  \", ratio of zeros: %.4f\" % (zeros_norm_num_list[seq] / entire_norm_num_list[seq]), \n",
    "                \", max zeros num: %d / %d\" % (max_zeros_num_list[seq][0], max_zeros_num_list[seq][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 디스크립터가 0가 나오는 경우가 매우 줄었다. 그럼이제 다시 최적의 파라미터값을 찾는걸 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = argparse.Namespace(\n",
    "    fpfh_normal_radiuse = 0.5,\n",
    "    fpfh_descriptors_radiuse = 1.25,\n",
    "    seq_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    mdgat_path = './KITTI',\n",
    "    kitti_path = '/media/vision/Seagate/DataSets/kitti/dataset/sequences',\n",
    "    memory_is_enough = True,\n",
    "    mutual_check = True,\n",
    "    transform_opt = 0\n",
    ")\n",
    "data_set = mdgat_kitti_data_set(opt)\n",
    "data = data_set.get_pair_data(0)\n",
    "\n",
    "desc0 = data['descriptors0']\n",
    "desc1 = data['descriptors1']\n",
    "match0 = data['gt_matches0']\n",
    "match1 = data['gt_matches1']\n",
    "\n",
    "for idx, matched_idx in enumerate(match0):\n",
    "    if matched_idx != -1:\n",
    "        print('matched_idx: ', idx, ' & ', matched_idx, ' || distance: ', np.linalg.norm(desc0[idx] - desc1[matched_idx]))\n",
    "        distance = np.linalg.norm(desc0 - desc1[matched_idx], axis=1)\n",
    "        print('distance with matched points: ', distance[idx], ' || similarity rank: ', np.argsort(distance).tolist().index(idx), '/', len(distance))\n",
    "        print('min distance: ', np.min(distance))\n",
    "        print('mean distance: ', np.mean(distance))\n",
    "        print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def extract_fpfh(pcd, voxel_size):\n",
    "  radius_normal = voxel_size * 2\n",
    "  pcd.estimate_normals(\n",
    "      o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "  radius_feature = voxel_size * 5\n",
    "  fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "      pcd, o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "  return np.array(fpfh.data).T\n",
    "\n",
    "def pcd2xyz(pcd):\n",
    "    return np.asarray(pcd.points).T\n",
    "\n",
    "def find_knn_cpu(feat0, feat1, knn=1, return_distance=False):\n",
    "    feat1tree = cKDTree(feat1)\n",
    "    dists, nn_inds = feat1tree.query(feat0, k=knn)\n",
    "    if return_distance:\n",
    "        return nn_inds, dists\n",
    "    else:\n",
    "        return nn_inds\n",
    "    \n",
    "def find_correspondences1(feats0, feats1):\n",
    "    dists = cdist(feats0, feats1)\n",
    "\n",
    "    '''Find ground true keypoint matching'''\n",
    "    min1 = np.argmin(dists, axis=0)\n",
    "    min2 = np.argmin(dists, axis=1)\n",
    "    min1v = np.min(dists, axis=1)\n",
    "    min1f = min2[min1v < 0.5]\n",
    "\n",
    "    match1, match2 = -2 * np.ones((len(feats0)), dtype=np.int16), -2 * np.ones((len(feats1)), dtype=np.int16)\n",
    "    xx = np.where(min2[min1] == np.arange(min1.shape[0]))[0]\n",
    "    matches = np.intersect1d(min1f, xx)\n",
    "\n",
    "    match1[min1[matches]] = matches\n",
    "    match2[matches] = min1[matches]\n",
    "\n",
    "    return match1, match2\n",
    "\n",
    "def find_correspondences(feats0, feats1, mutual_filter=True):\n",
    "    \"\"\"\n",
    "    Find correspondences between two sets of features.\n",
    "\n",
    "    Args:\n",
    "        feats0 (numpy.ndarray): Array of features from the first set.\n",
    "        feats1 (numpy.ndarray): Array of features from the second set.\n",
    "        mutual_filter (bool, optional): Whether to apply mutual filtering. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two numpy arrays: corres_idx0 and corres_idx1.\n",
    "               corres_idx0 (numpy.ndarray): Array of indices of correspondences in feats0.\n",
    "               corres_idx1 (numpy.ndarray): Array of indices of correspondences in feats1.\n",
    "    \"\"\"\n",
    "    # print(feats0.shape, feats1.shape)\n",
    "    nns01 = find_knn_cpu(feats0, feats1, knn=1, return_distance=False)\n",
    "    # print(nns01.shape)\n",
    "    corres01_idx0 = np.arange(len(nns01))\n",
    "    corres01_idx1 = nns01\n",
    "\n",
    "    if not mutual_filter:\n",
    "        return corres01_idx0, corres01_idx1\n",
    "\n",
    "    nns10 = find_knn_cpu(feats1, feats0, knn=1, return_distance=False)\n",
    "    corres10_idx1 = np.arange(len(nns10))\n",
    "    corres10_idx0 = nns10\n",
    "\n",
    "    # print(corres01_idx0, corres01_idx1)\n",
    "\n",
    "    match1, match2 = -1 * np.ones((len(feats0)), dtype=np.int16), -1 * np.ones((len(feats1)), dtype=np.int16)\n",
    "\n",
    "    mutual_filter = (corres10_idx0[corres01_idx1] == corres01_idx0)\n",
    "    # print(mutual_filter)\n",
    "    corres01_idx0[~mutual_filter] = -2\n",
    "    corres01_idx1[~mutual_filter] = -2\n",
    "    # print(corres01_idx0)\n",
    "    \n",
    "    # corres_idx0 = corres01_idx0[mutual_filter]\n",
    "    # print(corres_idx0)\n",
    "    # corres_idx1 = corres01_idx1[mutual_filter]\n",
    "\n",
    "    return corres01_idx0, corres01_idx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112045/262698142.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fpfh_desc0 = torch.tensor(self.comute_FPFH(pc0, kp0), dtype=torch.double)\n",
      "/tmp/ipykernel_112045/262698142.py:227: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fpfh_desc0 = torch.tensor(self.comute_FPFH(pc1, kp1), dtype=torch.double)\n",
      "/tmp/ipykernel_112045/262698142.py:272: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fpfh_desc0 = torch.tensor(self.comute_FPFH(pc0, kp0), dtype=torch.double)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 33]) (256, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112045/262698142.py:273: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fpfh_desc0 = torch.tensor(self.comute_FPFH(pc1, kp1), dtype=torch.double)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cdist(): argument 'x2' (position 2) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# np.set_printoptions(threshold=np.inf)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(fpfh_desc0\u001b[38;5;241m.\u001b[39mshape, fpfh_desc1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 36\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpfh_desc0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpfh_desc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(distances)\n\u001b[1;32m     38\u001b[0m match0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(distances, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/mdgat/lib/python3.10/site-packages/torch/functional.py:1214\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   1212\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode)\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cdist(): argument 'x2' (position 2) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# test_fpfh_normal_radiuses = [0.4, 0.5, 0.6, 0.7]\n",
    "# test_fpfh_descriptors_radiuses = [0.7 ,1.0, 1.2, 1.5]\n",
    "test_fpfh_normal_radiuses = [1.0]\n",
    "test_fpfh_descriptors_radiuses = [1.5]\n",
    "# test_fpfh_descriptors_radiuses = [1.5]\n",
    "\n",
    "opt = argparse.Namespace(\n",
    "    fpfh_normal_radiuse = 1.0,\n",
    "    fpfh_descriptors_radiuse = 1.5,\n",
    "    seq_list = [0, 2],\n",
    "    mdgat_path = './KITTI',\n",
    "    kitti_path = '/media/vision/Seagate/DataSets/kitti/dataset/sequences',\n",
    "    memory_is_enough = True,\n",
    "    mutual_check = False,\n",
    "    transform_opt = 0\n",
    ")\n",
    "data_set = mdgat_kitti_data_set(opt=opt)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(0, len(data_set), 100000):\n",
    "    data = data_set.get_pair_data(i)\n",
    "    kp0 = data['keypoints0']\n",
    "    kp1 = data['keypoints1']\n",
    "    desc0 = data['descriptors0']\n",
    "    desc1 = data['descriptors1']\n",
    "    fpfh_desc0 = data['fpfh_descriptors0']\n",
    "    fpfh_desc1 = data['fpfh_descriptors1']\n",
    "    match0_gt = data['gt_matches0']\n",
    "    match1_gt = data['gt_matches1']\n",
    "\n",
    "    # np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "    print(fpfh_desc0.shape, fpfh_desc1.shape)\n",
    "\n",
    "    distances = torch.cdist(fpfh_desc0, fpfh_desc1, p=2)\n",
    "    print(distances)\n",
    "    match0 = torch.argmin(distances, dim=1).numpy()\n",
    "    match1 = torch.argmin(distances, dim=0).numpy()\n",
    "    print(\"match0:\", match0.shape, match0_gt.shape)\n",
    "    print(\"match1:\", match)\n",
    "\n",
    "    corres0 = match0 == match0_gt\n",
    "    corres1 = match1 == match1_gt\n",
    "\n",
    "    print(fpfh_desc0)\n",
    "\n",
    "    print(\"corres0:\", corres0)\n",
    "    print(\"corres1:\", corres1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
